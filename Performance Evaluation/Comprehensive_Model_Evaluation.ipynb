{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive ConvNeXt Model Evaluation\n",
    "\n",
    "**Project Phoenix - Cervical Cancer Cell Classification**\n",
    "\n",
    "This notebook provides an extensive evaluation of the fine-tuned ConvNeXt model on the Sipakmed dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Dependencies](#setup)\n",
    "2. [Model & Data Loading](#loading)\n",
    "3. [Basic Performance Metrics](#basic-metrics)\n",
    "4. [Confusion Matrix Analysis](#confusion-matrix)\n",
    "5. [ROC Curves & AUC Scores](#roc-curves)\n",
    "6. [Precision-Recall Curves](#pr-curves)\n",
    "7. [Per-Class Performance Analysis](#per-class)\n",
    "8. [Error Analysis](#error-analysis)\n",
    "9. [Confidence Score Distribution](#confidence)\n",
    "10. [Top-K Accuracy Analysis](#topk)\n",
    "11. [Advanced Statistical Metrics](#advanced-metrics)\n",
    "12. [Sample Predictions Visualization](#samples)\n",
    "13. [Summary Report](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if running in a fresh environment)\n",
    "# Uncomment the following line if needed:\n",
    "# !pip install transformers datasets torch torchvision scikit-learn matplotlib seaborn pandas numpy pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Deep Learning & Transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoImageProcessor, ConvNextV2ForImageClassification\n",
    "from datasets import Dataset, DatasetDict, Features, ClassLabel, Image as HFImage\n",
    "\n",
    "# Data Processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    cohen_kappa_score, matthews_corrcoef,\n",
    "    balanced_accuracy_score, top_k_accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model & Data Loading <a id='loading'></a>\n",
    "\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "# Update these paths according to your setup\n",
    "\n",
    "# Path to your trained model checkpoint\n",
    "# This should contain: config.json, model.safetensors, preprocessor_config.json, etc.\n",
    "MODEL_PATH = \"./path/to/your/model/checkpoint\"  # UPDATE THIS!\n",
    "\n",
    "# Path to your Sipakmed dataset\n",
    "# Should contain subdirectories for each class with NLM_CLAHE processed images\n",
    "DATASET_PATH = Path(\"./path/to/sipakmed/dataset\")  # UPDATE THIS!\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Batch size for inference\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Class names (should match your training)\n",
    "CLASS_NAMES = [\n",
    "    'im_Dyskeratotic',\n",
    "    'im_Koilocytotic',\n",
    "    'im_Metaplastic',\n",
    "    'im_Parabasal',\n",
    "    'im_Superficial-Intermediate'\n",
    "]\n",
    "\n",
    "# Results output directory\n",
    "OUTPUT_DIR = Path(\"./evaluation_results\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Model Path: {MODEL_PATH}\")\n",
    "print(f\"Dataset Path: {DATASET_PATH}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model and processor...\")\n",
    "\n",
    "# Load the image processor\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_PATH)\n",
    "print(f\"✓ Processor loaded\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = ConvNextV2ForImageClassification.from_pretrained(MODEL_PATH)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(f\"✓ Model loaded and moved to {DEVICE}\")\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  - Number of classes: {model.config.num_labels}\")\n",
    "print(f\"  - Image size: {model.config.image_size}\")\n",
    "print(f\"  - Hidden sizes: {model.config.hidden_sizes}\")\n",
    "print(f\"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  - Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading and preparing dataset...\")\n",
    "\n",
    "# Find all NLM_CLAHE directories\n",
    "nlm_dirs = set()\n",
    "\n",
    "# Search for NLM_CLAHE directories\n",
    "for child in DATASET_PATH.iterdir():\n",
    "    if not child.is_dir():\n",
    "        continue\n",
    "    for sub in child.iterdir():\n",
    "        if sub.is_dir() and sub.name.lower() == \"nlm_clahe\":\n",
    "            nlm_dirs.add(sub.resolve())\n",
    "            break\n",
    "\n",
    "# Recursive fallback\n",
    "for p in DATASET_PATH.rglob(\"*\"):\n",
    "    if p.is_dir() and p.name.lower() == \"nlm_clahe\":\n",
    "        nlm_dirs.add(p.resolve())\n",
    "\n",
    "if not nlm_dirs:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No 'NLM_CLAHE' directories found under {DATASET_PATH}. \"\n",
    "        \"Please check your dataset path.\"\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(nlm_dirs)} NLM_CLAHE directories\")\n",
    "\n",
    "# Collect all images\n",
    "rows = []\n",
    "seen_paths = set()\n",
    "\n",
    "for nlm in sorted(nlm_dirs):\n",
    "    class_name = nlm.parent.name\n",
    "    bmp_files = [p.resolve() for p in nlm.iterdir() if p.is_file() and p.suffix.lower() == \".bmp\"]\n",
    "    \n",
    "    if not bmp_files:\n",
    "        print(f\"Warning: No .bmp files found in {nlm} (class: '{class_name}')\")\n",
    "        continue\n",
    "    \n",
    "    for p in bmp_files:\n",
    "        sp = str(p)\n",
    "        if sp not in seen_paths:\n",
    "            seen_paths.add(sp)\n",
    "            rows.append((sp, class_name))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows, columns=[\"image_path\", \"label_name\"])\n",
    "\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"No .bmp files found in any NLM_CLAHE directories\")\n",
    "\n",
    "# Create label mapping\n",
    "class_names_sorted = sorted(df[\"label_name\"].unique().tolist())\n",
    "label_to_id = {n: i for i, n in enumerate(class_names_sorted)}\n",
    "id_to_label = {i: n for n, i in label_to_id.items()}\n",
    "df[\"label\"] = df[\"label_name\"].map(label_to_id)\n",
    "\n",
    "# Shuffle\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  - Total images: {len(df)}\")\n",
    "print(f\"  - Number of classes: {len(class_names_sorted)}\")\n",
    "print(f\"  - Classes: {class_names_sorted}\")\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(df[\"label_name\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test split (matching training split)\n",
    "print(\"Creating data splits...\")\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(val_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n",
    "\n",
    "# We'll primarily use the test set for evaluation\n",
    "eval_df = test_df.reset_index(drop=True)\n",
    "print(f\"\\nUsing test set ({len(eval_df)} images) for comprehensive evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace Dataset\n",
    "print(\"Creating HuggingFace Dataset...\")\n",
    "\n",
    "features = Features({\n",
    "    \"image\": HFImage(),\n",
    "    \"label\": ClassLabel(names=class_names_sorted)\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"image\": eval_df[\"image_path\"].tolist(),\n",
    "    \"label\": eval_df[\"label\"].tolist()\n",
    "}).cast(features)\n",
    "\n",
    "print(f\"✓ Dataset created with {len(eval_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing transforms\n",
    "mean, std = processor.image_mean, processor.image_std\n",
    "\n",
    "eval_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "def apply_transforms(examples):\n",
    "    \"\"\"Apply transformations to images\"\"\"\n",
    "    examples[\"pixel_values\"] = [\n",
    "        eval_transform(img.convert(\"RGB\")) for img in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "# Apply transforms\n",
    "eval_dataset = eval_dataset.map(\n",
    "    apply_transforms, \n",
    "    batched=True, \n",
    "    remove_columns=[\"image\"]\n",
    ")\n",
    "\n",
    "print(\"✓ Transforms applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running inference on test set...\")\n",
    "\n",
    "all_predictions = []\n",
    "all_probabilities = []\n",
    "all_labels = []\n",
    "\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.tensor([item['label'] for item in batch])\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    eval_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        pixel_values = batch['pixel_values'].to(DEVICE)\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "        predictions = logits.argmax(dim=-1).cpu().numpy()\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "        all_probabilities.extend(probs)\n",
    "        all_labels.extend(labels.numpy())\n",
    "        \n",
    "        if (batch_idx + 1) % 5 == 0:\n",
    "            print(f\"  Processed {(batch_idx + 1) * BATCH_SIZE}/{len(eval_dataset)} samples\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "print(f\"\\n✓ Inference complete!\")\n",
    "print(f\"  Predictions shape: {all_predictions.shape}\")\n",
    "print(f\"  Probabilities shape: {all_probabilities.shape}\")\n",
    "print(f\"  Labels shape: {all_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Performance Metrics <a id='basic-metrics'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision_macro = precision_score(all_labels, all_predictions, average='macro')\n",
    "precision_weighted = precision_score(all_labels, all_predictions, average='weighted')\n",
    "recall_macro = recall_score(all_labels, all_predictions, average='macro')\n",
    "recall_weighted = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
    "f1_weighted = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OVERALL PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAccuracy:              {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"\\nPrecision (Macro):     {precision_macro:.4f}\")\n",
    "print(f\"Precision (Weighted):  {precision_weighted:.4f}\")\n",
    "print(f\"\\nRecall (Macro):        {recall_macro:.4f}\")\n",
    "print(f\"Recall (Weighted):     {recall_weighted:.4f}\")\n",
    "print(f\"\\nF1-Score (Macro):      {f1_macro:.4f}\")\n",
    "print(f\"F1-Score (Weighted):   {f1_weighted:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overall metrics\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Macro metrics\n",
    "metrics_macro = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision_macro,\n",
    "    'Recall': recall_macro,\n",
    "    'F1-Score': f1_macro\n",
    "}\n",
    "\n",
    "colors_macro = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12']\n",
    "bars1 = ax[0].bar(metrics_macro.keys(), metrics_macro.values(), color=colors_macro, alpha=0.8)\n",
    "ax[0].set_ylim([0, 1.0])\n",
    "ax[0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax[0].set_title('Overall Performance Metrics (Macro Average)', fontsize=14, fontweight='bold')\n",
    "ax[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.4f}',\n",
    "               ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Weighted metrics\n",
    "metrics_weighted = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision_weighted,\n",
    "    'Recall': recall_weighted,\n",
    "    'F1-Score': f1_weighted\n",
    "}\n",
    "\n",
    "bars2 = ax[1].bar(metrics_weighted.keys(), metrics_weighted.values(), color=colors_macro, alpha=0.8)\n",
    "ax[1].set_ylim([0, 1.0])\n",
    "ax[1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax[1].set_title('Overall Performance Metrics (Weighted Average)', fontsize=14, fontweight='bold')\n",
    "ax[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.4f}',\n",
    "               ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'overall_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {OUTPUT_DIR / 'overall_metrics.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix Analysis <a id='confusion-matrix'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "cm_normalized = confusion_matrix(all_labels, all_predictions, normalize='true')\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names_sorted,\n",
    "            yticklabels=class_names_sorted,\n",
    "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('Confusion Matrix (Raw Counts)', fontsize=16, fontweight='bold', pad=20)\n",
    "axes[0].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Plot 2: Normalized (percentages)\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens',\n",
    "            xticklabels=class_names_sorted,\n",
    "            yticklabels=class_names_sorted,\n",
    "            ax=axes[1], cbar_kws={'label': 'Percentage'})\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold', pad=20)\n",
    "axes[1].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].tick_params(axis='y', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {OUTPUT_DIR / 'confusion_matrices.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassifications from confusion matrix\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, true_class in enumerate(class_names_sorted):\n",
    "    print(f\"\\n{true_class}:\")\n",
    "    print(f\"  Correctly classified: {cm[i, i]} / {cm[i].sum()} ({cm_normalized[i, i]*100:.2f}%)\")\n",
    "    \n",
    "    # Find main misclassifications\n",
    "    misclassified_indices = np.argsort(cm[i])[::-1]\n",
    "    for j in misclassified_indices:\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            print(f\"  Misclassified as {class_names_sorted[j]}: {cm[i, j]} ({cm_normalized[i, j]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ROC Curves & AUC Scores <a id='roc-curves'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize labels for multi-class ROC\n",
    "n_classes = len(class_names_sorted)\n",
    "y_true_bin = label_binarize(all_labels, classes=range(n_classes))\n",
    "\n",
    "# Calculate ROC curve and AUC for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], all_probabilities[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Calculate micro-average ROC curve and AUC\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), all_probabilities.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Calculate macro-average ROC curve and AUC\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "print(\"ROC AUC Scores:\")\n",
    "print(f\"  Micro-average: {roc_auc['micro']:.4f}\")\n",
    "print(f\"  Macro-average: {roc_auc['macro']:.4f}\")\n",
    "for i, class_name in enumerate(class_names_sorted):\n",
    "    print(f\"  {class_name}: {roc_auc[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Define colors\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, n_classes))\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "for i, color, class_name in zip(range(n_classes), colors, class_names_sorted):\n",
    "    ax.plot(fpr[i], tpr[i], color=color, lw=2.5, alpha=0.8,\n",
    "            label=f'{class_name} (AUC = {roc_auc[i]:.3f})')\n",
    "\n",
    "# Plot micro and macro averages\n",
    "ax.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "        label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})',\n",
    "        color='deeppink', linestyle='--', linewidth=3)\n",
    "\n",
    "ax.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "        label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.3f})',\n",
    "        color='navy', linestyle='--', linewidth=3)\n",
    "\n",
    "# Plot diagonal line (random classifier)\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
    "ax.set_title('ROC Curves - Multi-Class (One-vs-Rest)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc=\"lower right\", fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {OUTPUT_DIR / 'roc_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Precision-Recall Curves <a id='pr-curves'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Precision-Recall curve for each class\n",
    "precision_dict = dict()\n",
    "recall_dict = dict()\n",
    "avg_precision = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    precision_dict[i], recall_dict[i], _ = precision_recall_curve(\n",
    "        y_true_bin[:, i], all_probabilities[:, i]\n",
    "    )\n",
    "    avg_precision[i] = average_precision_score(y_true_bin[:, i], all_probabilities[:, i])\n",
    "\n",
    "# Micro-average\n",
    "precision_dict[\"micro\"], recall_dict[\"micro\"], _ = precision_recall_curve(\n",
    "    y_true_bin.ravel(), all_probabilities.ravel()\n",
    ")\n",
    "avg_precision[\"micro\"] = average_precision_score(y_true_bin, all_probabilities, average=\"micro\")\n",
    "\n",
    "# Macro-average\n",
    "avg_precision[\"macro\"] = average_precision_score(y_true_bin, all_probabilities, average=\"macro\")\n",
    "\n",
    "print(\"Average Precision Scores:\")\n",
    "print(f\"  Micro-average: {avg_precision['micro']:.4f}\")\n",
    "print(f\"  Macro-average: {avg_precision['macro']:.4f}\")\n",
    "for i, class_name in enumerate(class_names_sorted):\n",
    "    print(f\"  {class_name}: {avg_precision[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot PR curve for each class\n",
    "for i, color, class_name in zip(range(n_classes), colors, class_names_sorted):\n",
    "    ax.plot(recall_dict[i], precision_dict[i], color=color, lw=2.5, alpha=0.8,\n",
    "            label=f'{class_name} (AP = {avg_precision[i]:.3f})')\n",
    "\n",
    "# Plot micro-average\n",
    "ax.plot(recall_dict[\"micro\"], precision_dict[\"micro\"],\n",
    "        label=f'Micro-average (AP = {avg_precision[\"micro\"]:.3f})',\n",
    "        color='deeppink', linestyle='--', linewidth=3)\n",
    "\n",
    "# Plot baseline (random classifier for balanced dataset)\n",
    "baseline = y_true_bin.sum() / len(y_true_bin)\n",
    "ax.plot([0, 1], [baseline, baseline], 'k--', lw=2, \n",
    "        label=f'Random Classifier (AP = {baseline:.3f})')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('Recall', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Precision', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Precision-Recall Curves - Multi-Class (One-vs-Rest)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc=\"lower left\", fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'precision_recall_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {OUTPUT_DIR / 'precision_recall_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Per-Class Performance Analysis <a id='per-class'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification report\n",
    "report = classification_report(\n",
    "    all_labels, \n",
    "    all_predictions, \n",
    "    target_names=class_names_sorted,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(report_df.to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save to CSV\n",
    "report_df.to_csv(OUTPUT_DIR / 'classification_report.csv')\n",
    "print(f\"\\n✓ Saved: {OUTPUT_DIR / 'classification_report.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class metrics\n",
    "class_metrics = report_df.iloc[:-3]  # Exclude macro avg, weighted avg, accuracy rows\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Per-Class Performance Metrics', fontsize=18, fontweight='bold', y=1.00)\n",
    "\n",
    "metrics_to_plot = ['precision', 'recall', 'f1-score', 'support']\n",
    "metric_titles = ['Precision', 'Recall', 'F1-Score', 'Support (# Samples)']\n",
    "metric_colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, (metric, title, color) in enumerate(zip(metrics_to_plot, metric_titles, metric_colors)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    values = class_metrics[metric].values\n",
    "    bars = ax.barh(class_names_sorted, values, color=color, alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        width = bar.get_width()\n",
    "        label_format = f'{val:.3f}' if metric != 'support' else f'{int(val)}'\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                f' {label_format}',\n",
    "                ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{title} by Class', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    if metric != 'support':\n",
    "        ax.set_xlim([0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {OUTPUT_DIR / 'per_class_metrics.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis <a id='error-analysis'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify misclassified samples\n",
    "misclassified_mask = all_predictions != all_labels\n",
    "misclassified_indices = np.where(misclassified_mask)[0]\n",
    "correctly_classified_indices = np.where(~misclassified_mask)[0]\n",
    "\n",
    "print(f\"Total samples: {len(all_labels)}\")\n",
    "print(f\"Correctly classified: {len(correctly_classified_indices)} ({len(correctly_classified_indices)/len(all_labels)*100:.2f}%)\")\n",
    "print(f\"Misclassified: {len(misclassified_indices)} ({len(misclassified_indices)/len(all_labels)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassification patterns\n",
    "misclassification_patterns = []\n",
    "\n",
    "for idx in misclassified_indices:\n",
    "    true_label = all_labels[idx]\n",
    "    pred_label = all_predictions[idx]\n",
    "    confidence = all_probabilities[idx, pred_label]\n",
    "    true_prob = all_probabilities[idx, true_label]\n",
    "    \n",
    "    misclassification_patterns.append({\n",
    "        'index': idx,\n",
    "        'true_class': class_names_sorted[true_label],\n",
    "        'predicted_class': class_names_sorted[pred_label],\n",
    "        'confidence': confidence,\n",
    "        'true_class_prob': true_prob,\n",
    "        'margin': confidence - true_prob\n",
    "    })\n",
    "\n",
    "misclass_df = pd.DataFrame(misclassification_patterns)\n",
    "\n",
    "# Show top misclassifications by confidence\n",
    "print(\"\\nTop 10 Most Confident Misclassifications:\")\n",
    "print(misclass_df.nlargest(10, 'confidence')[['true_class', 'predicted_class', 'confidence', 'true_class_prob']].to_string())\n",
    "\n",
    "# Save misclassifications\n",
    "misclass_df.to_csv(OUTPUT_DIR / 'misclassifications.csv', index=False)\n",
    "print(f\"\\n✓ Saved: {OUTPUT_DIR / 'misclassifications.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize misclassification patterns\n",
    "misclass_matrix = pd.crosstab(\n",
    "    misclass_df['true_class'], \n",
    "    misclass_df['predicted_class']\n",
    ").reindex(index=class_names_sorted, columns=class_names_sorted, fill_value=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(misclass_matrix, annot=True, fmt='d', cmap='Reds', \n",
    "            xticklabels=class_names_sorted,\n",
    "            yticklabels=class_names_sorted,\n",
    "            ax=ax, cbar_kws={'label': 'Count'})\n",
    "ax.set_title('Misclassification Patterns', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'misclassification_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {OUTPUT_DIR / 'misclassification_patterns.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Confidence Score Distribution <a id='confidence'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confidence scores (max probability)\n",
    "confidence_scores = np.max(all_probabilities, axis=1)\n",
    "\n",
    "# Separate correct and incorrect predictions\n",
    "correct_confidences = confidence_scores[~misclassified_mask]\n",
    "incorrect_confidences = confidence_scores[misclassified_mask]\n",
    "\n",
    "print(f\"Average confidence (correct predictions): {correct_confidences.mean():.4f} ± {correct_confidences.std():.4f}\")\n",
    "print(f\"Average confidence (incorrect predictions): {incorrect_confidences.mean():.4f} ± {incorrect_confidences.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Histogram\n",
    "axes[0].hist(correct_confidences, bins=50, alpha=0.7, label='Correct', color='green', edgecolor='black')\n",
    "axes[0].hist(incorrect_confidences, bins=50, alpha=0.7, label='Incorrect', color='red', edgecolor='black')\n",
    "axes[0].set_xlabel('Confidence Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Confidence Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Box plot by class\n",
    "confidence_by_class = []\n",
    "for i, class_name in enumerate(class_names_sorted):\n",
    "    class_mask = all_labels == i\n",
    "    confidence_by_class.append(confidence_scores[class_mask])\n",
    "\n",
    "bp = axes[1].boxplot(confidence_by_class, labels=class_names_sorted, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "axes[1].set_ylabel('Confidence Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Confidence Distribution by True Class', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'confidence_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {OUTPUT_DIR / 'confidence_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Top-K Accuracy Analysis <a id='topk'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate top-k accuracy for k = 1, 2, 3\n",
    "topk_accuracies = []\n",
    "\n",
    "for k in range(1, min(n_classes + 1, 4)):\n",
    "    top_k_acc = top_k_accuracy_score(all_labels, all_probabilities, k=k)\n",
    "    topk_accuracies.append(top_k_acc)\n",
    "    print(f\"Top-{k} Accuracy: {top_k_acc:.4f} ({top_k_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top-k accuracy\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "k_values = list(range(1, len(topk_accuracies) + 1))\n",
    "bars = ax.bar(k_values, topk_accuracies, color=['#2ecc71', '#3498db', '#f39c12'], alpha=0.8)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, topk_accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.4f}\\n({acc*100:.2f}%)',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('K', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top-K Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(k_values)\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'topk_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {OUTPUT_DIR / 'topk_accuracy.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced Statistical Metrics <a id='advanced-metrics'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate advanced metrics\n",
    "kappa = cohen_kappa_score(all_labels, all_predictions)\n",
    "mcc = matthews_corrcoef(all_labels, all_predictions)\n",
    "balanced_acc = balanced_accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "# Calculate sensitivity and specificity for each class\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # Binary classification: class i vs rest\n",
    "    true_binary = (all_labels == i).astype(int)\n",
    "    pred_binary = (all_predictions == i).astype(int)\n",
    "    \n",
    "    tn = np.sum((true_binary == 0) & (pred_binary == 0))\n",
    "    tp = np.sum((true_binary == 1) & (pred_binary == 1))\n",
    "    fn = np.sum((true_binary == 1) & (pred_binary == 0))\n",
    "    fp = np.sum((true_binary == 0) & (pred_binary == 1))\n",
    "    \n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ADVANCED STATISTICAL METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCohen's Kappa:          {kappa:.4f}\")\n",
    "print(f\"Matthews Corr. Coef:    {mcc:.4f}\")\n",
    "print(f\"Balanced Accuracy:      {balanced_acc:.4f}\")\n",
    "print(f\"\\nAverage Sensitivity:    {np.mean(sensitivities):.4f}\")\n",
    "print(f\"Average Specificity:    {np.mean(specificities):.4f}\")\n",
    "print(\"\\nPer-Class Sensitivity & Specificity:\")\n",
    "for i, class_name in enumerate(class_names_sorted):\n",
    "    print(f\"  {class_name}:\")\n",
    "    print(f\"    Sensitivity: {sensitivities[i]:.4f}\")\n",
    "    print(f\"    Specificity: {specificities[i]:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensitivity and specificity\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "x = np.arange(len(class_names_sorted))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, sensitivities, width, label='Sensitivity', color='#3498db', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, specificities, width, label='Specificity', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Sensitivity and Specificity by Class', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names_sorted, rotation=45, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'sensitivity_specificity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {OUTPUT_DIR / 'sensitivity_specificity.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sample Predictions Visualization <a id='samples'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions\n",
    "# Show 3 correct predictions and 3 incorrect predictions per class (if available)\n",
    "\n",
    "def denormalize_image(tensor, mean, std):\n",
    "    \"\"\"Denormalize image tensor for visualization\"\"\"\n",
    "    img = tensor.clone()\n",
    "    for t, m, s in zip(img, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return img.clamp(0, 1)\n",
    "\n",
    "# For each class, get some correct and incorrect predictions\n",
    "n_samples_per_class = 2\n",
    "\n",
    "for class_idx, class_name in enumerate(class_names_sorted):\n",
    "    print(f\"\\nGenerating visualizations for class: {class_name}\")\n",
    "    \n",
    "    # Find correct predictions\n",
    "    correct_mask = (all_labels == class_idx) & (all_predictions == class_idx)\n",
    "    correct_indices = np.where(correct_mask)[0]\n",
    "    \n",
    "    # Find incorrect predictions\n",
    "    incorrect_mask = (all_labels == class_idx) & (all_predictions != class_idx)\n",
    "    incorrect_indices = np.where(incorrect_mask)[0]\n",
    "    \n",
    "    # Sample indices\n",
    "    n_correct = min(n_samples_per_class, len(correct_indices))\n",
    "    n_incorrect = min(n_samples_per_class, len(incorrect_indices))\n",
    "    \n",
    "    if n_correct == 0 and n_incorrect == 0:\n",
    "        print(f\"  No samples found for {class_name}\")\n",
    "        continue\n",
    "    \n",
    "    selected_correct = np.random.choice(correct_indices, n_correct, replace=False) if n_correct > 0 else []\n",
    "    selected_incorrect = np.random.choice(incorrect_indices, n_incorrect, replace=False) if n_incorrect > 0 else []\n",
    "    \n",
    "    # Create figure\n",
    "    n_total = n_correct + n_incorrect\n",
    "    fig, axes = plt.subplots(1, n_total, figsize=(5*n_total, 5))\n",
    "    if n_total == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    # Plot correct predictions\n",
    "    for sample_idx in selected_correct:\n",
    "        # Get image\n",
    "        pixel_values = eval_dataset[sample_idx]['pixel_values']\n",
    "        img = denormalize_image(pixel_values, mean, std)\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        axes[idx].imshow(img_np)\n",
    "        axes[idx].set_title(f\"✓ CORRECT\\nTrue: {class_name}\\nConf: {all_probabilities[sample_idx, class_idx]:.3f}\",\n",
    "                           color='green', fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "        idx += 1\n",
    "    \n",
    "    # Plot incorrect predictions\n",
    "    for sample_idx in selected_incorrect:\n",
    "        # Get image\n",
    "        pixel_values = eval_dataset[sample_idx]['pixel_values']\n",
    "        img = denormalize_image(pixel_values, mean, std)\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        pred_class = all_predictions[sample_idx]\n",
    "        axes[idx].imshow(img_np)\n",
    "        axes[idx].set_title(f\"✗ INCORRECT\\nTrue: {class_name}\\nPred: {class_names_sorted[pred_class]}\\nConf: {all_probabilities[sample_idx, pred_class]:.3f}\",\n",
    "                           color='red', fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "        idx += 1\n",
    "    \n",
    "    plt.suptitle(f'Sample Predictions for {class_name}', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / f'samples_{class_name}.png', dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"  ✓ Saved: {OUTPUT_DIR / f'samples_{class_name}.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary Report <a id='summary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "summary_report = f\"\"\"\n",
    "{'='*80}\n",
    "COMPREHENSIVE MODEL EVALUATION SUMMARY REPORT\n",
    "{'='*80}\n",
    "\n",
    "Model: ConvNeXt V2 Fine-tuned on Sipakmed Dataset\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Test Set Size: {len(all_labels)} samples\n",
    "Number of Classes: {n_classes}\n",
    "\n",
    "{'='*80}\n",
    "OVERALL PERFORMANCE\n",
    "{'='*80}\n",
    "\n",
    "Accuracy:                    {accuracy:.4f} ({accuracy*100:.2f}%)\n",
    "Balanced Accuracy:           {balanced_acc:.4f}\n",
    "\n",
    "Precision (Macro):           {precision_macro:.4f}\n",
    "Precision (Weighted):        {precision_weighted:.4f}\n",
    "\n",
    "Recall (Macro):              {recall_macro:.4f}\n",
    "Recall (Weighted):           {recall_weighted:.4f}\n",
    "\n",
    "F1-Score (Macro):            {f1_macro:.4f}\n",
    "F1-Score (Weighted):         {f1_weighted:.4f}\n",
    "\n",
    "{'='*80}\n",
    "ADVANCED METRICS\n",
    "{'='*80}\n",
    "\n",
    "Cohen's Kappa:               {kappa:.4f}\n",
    "Matthews Correlation Coef:   {mcc:.4f}\n",
    "\n",
    "ROC AUC (Micro):             {roc_auc['micro']:.4f}\n",
    "ROC AUC (Macro):             {roc_auc['macro']:.4f}\n",
    "\n",
    "Avg Precision (Micro):       {avg_precision['micro']:.4f}\n",
    "Avg Precision (Macro):       {avg_precision['macro']:.4f}\n",
    "\n",
    "Average Sensitivity:         {np.mean(sensitivities):.4f}\n",
    "Average Specificity:         {np.mean(specificities):.4f}\n",
    "\n",
    "{'='*80}\n",
    "TOP-K ACCURACY\n",
    "{'='*80}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for k, acc in enumerate(topk_accuracies, 1):\n",
    "    summary_report += f\"Top-{k} Accuracy:              {acc:.4f} ({acc*100:.2f}%)\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "{'='*80}\n",
    "PER-CLASS PERFORMANCE\n",
    "{'='*80}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i, class_name in enumerate(class_names_sorted):\n",
    "    summary_report += f\"\"\"\n",
    "{class_name}:\n",
    "  Support:        {int(class_metrics.loc[class_name, 'support'])}\n",
    "  Precision:      {class_metrics.loc[class_name, 'precision']:.4f}\n",
    "  Recall:         {class_metrics.loc[class_name, 'recall']:.4f}\n",
    "  F1-Score:       {class_metrics.loc[class_name, 'f1-score']:.4f}\n",
    "  ROC AUC:        {roc_auc[i]:.4f}\n",
    "  Avg Precision:  {avg_precision[i]:.4f}\n",
    "  Sensitivity:    {sensitivities[i]:.4f}\n",
    "  Specificity:    {specificities[i]:.4f}\n",
    "\"\"\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "{'='*80}\n",
    "ERROR ANALYSIS\n",
    "{'='*80}\n",
    "\n",
    "Total Samples:               {len(all_labels)}\n",
    "Correctly Classified:        {len(correctly_classified_indices)} ({len(correctly_classified_indices)/len(all_labels)*100:.2f}%)\n",
    "Misclassified:               {len(misclassified_indices)} ({len(misclassified_indices)/len(all_labels)*100:.2f}%)\n",
    "\n",
    "Avg Confidence (Correct):    {correct_confidences.mean():.4f} ± {correct_confidences.std():.4f}\n",
    "Avg Confidence (Incorrect):  {incorrect_confidences.mean():.4f} ± {incorrect_confidences.std():.4f}\n",
    "\n",
    "{'='*80}\n",
    "FILES GENERATED\n",
    "{'='*80}\n",
    "\n",
    "1. overall_metrics.png - Overall performance metrics visualization\n",
    "2. confusion_matrices.png - Confusion matrices (raw and normalized)\n",
    "3. roc_curves.png - ROC curves for all classes\n",
    "4. precision_recall_curves.png - Precision-Recall curves\n",
    "5. per_class_metrics.png - Per-class performance metrics\n",
    "6. misclassification_patterns.png - Misclassification heatmap\n",
    "7. confidence_distribution.png - Confidence score distributions\n",
    "8. topk_accuracy.png - Top-K accuracy visualization\n",
    "9. sensitivity_specificity.png - Sensitivity and specificity by class\n",
    "10. samples_[class].png - Sample predictions for each class\n",
    "11. classification_report.csv - Detailed classification report\n",
    "12. misclassifications.csv - All misclassified samples\n",
    "13. evaluation_summary.txt - This summary report\n",
    "\n",
    "{'='*80}\n",
    "END OF REPORT\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save summary report\n",
    "with open(OUTPUT_DIR / 'evaluation_summary.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\n✓ Saved: {OUTPUT_DIR / 'evaluation_summary.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This comprehensive evaluation provides deep insights into the model's performance across multiple dimensions:\n",
    "\n",
    "✅ **Overall Performance**: High-level metrics show the model's general effectiveness\n",
    "\n",
    "✅ **Confusion Matrix**: Reveals specific class confusions and misclassification patterns\n",
    "\n",
    "✅ **ROC & PR Curves**: Demonstrate model's ability to discriminate between classes\n",
    "\n",
    "✅ **Per-Class Analysis**: Identifies strengths and weaknesses for each cell type\n",
    "\n",
    "✅ **Error Analysis**: Provides actionable insights for model improvement\n",
    "\n",
    "✅ **Confidence Analysis**: Shows model calibration and uncertainty\n",
    "\n",
    "✅ **Statistical Metrics**: Advanced measures for comprehensive assessment\n",
    "\n",
    "All results have been saved to the `evaluation_results` directory for further analysis and reporting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
