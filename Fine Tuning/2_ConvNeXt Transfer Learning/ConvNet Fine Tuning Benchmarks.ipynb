{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e71937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parent: c:\\Meet\\Projects\\Project_8_Phoenix_Cervical Cancer Image Classification\\Project-Phoenix\\Dataset\\Augmented Dataset - Limited Enhancement\n",
      "Discovered NLM_CLAHE directories (count): 5\n",
      "  - C:\\Meet\\Projects\\Project_8_Phoenix_Cervical Cancer Image Classification\\Project-Phoenix\\Dataset\\Augmented Dataset - Limited Enhancement\\im_Dyskeratotic\\NLM_CLAHE\n",
      "  - C:\\Meet\\Projects\\Project_8_Phoenix_Cervical Cancer Image Classification\\Project-Phoenix\\Dataset\\Augmented Dataset - Limited Enhancement\\im_Koilocytotic\\NLM_CLAHE\n",
      "  - C:\\Meet\\Projects\\Project_8_Phoenix_Cervical Cancer Image Classification\\Project-Phoenix\\Dataset\\Augmented Dataset - Limited Enhancement\\im_Metaplastic\\NLM_CLAHE\n",
      "  - C:\\Meet\\Projects\\Project_8_Phoenix_Cervical Cancer Image Classification\\Project-Phoenix\\Dataset\\Augmented Dataset - Limited Enhancement\\im_Parabasal\\NLM_CLAHE\n",
      "  - C:\\Meet\\Projects\\Project_8_Phoenix_Cervical Cancer Image Classification\\Project-Phoenix\\Dataset\\Augmented Dataset - Limited Enhancement\\im_Superficial-Intermediate\\NLM_CLAHE\n",
      "\n",
      "Found classes (alphabetical): ['im_Dyskeratotic', 'im_Koilocytotic', 'im_Metaplastic', 'im_Parabasal', 'im_Superficial-Intermediate']\n",
      "Total images found: 4049\n",
      "Counts per class:\n",
      "label_name\n",
      "im_Superficial-Intermediate    831\n",
      "im_Koilocytotic                825\n",
      "im_Dyskeratotic                813\n",
      "im_Metaplastic                 793\n",
      "im_Parabasal                   787\n",
      "dtype: int64\n",
      "\n",
      "Saved file list to: c:\\Meet\\Projects\\Project_8_Phoenix_Cervical Cancer Image Classification\\Project-Phoenix\\Dataset\\Augmented Dataset - Limited Enhancement\\sipakmed_file_list.csv\n"
     ]
    }
   ],
   "source": [
    "# --- dependencies ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_PARENT = Path(\n",
    "    r\"c:/Meet/Projects/Project_8_Phoenix_Cervical Cancer Image Classification/Project-Phoenix/Dataset/Augmented Dataset - Limited Enhancement\"\n",
    ")\n",
    "# ----------------------------------------------\n",
    "\n",
    "if not DATASET_PARENT.exists():\n",
    "    raise FileNotFoundError(f\"Dataset parent path not found: {DATASET_PARENT}\")\n",
    "\n",
    "# --- find all NLM_CLAHE directories (case-insensitive) ---\n",
    "nlm_dirs = set()\n",
    "\n",
    "# 1) immediate child search: look for X/<class>/NLM_CLAHE\n",
    "for child in DATASET_PARENT.iterdir():\n",
    "    if not child.is_dir():\n",
    "        continue\n",
    "    # search child for a folder named NLM_CLAHE (case-insensitive)\n",
    "    for sub in child.iterdir():\n",
    "        if sub.is_dir() and sub.name.lower() == \"nlm_clahe\":\n",
    "            nlm_dirs.add(sub.resolve())\n",
    "            break\n",
    "\n",
    "# 2) recursive fallback: in case structure is deeper or different\n",
    "for p in DATASET_PARENT.rglob(\"*\"):\n",
    "    if p.is_dir() and p.name.lower() == \"nlm_clahe\":\n",
    "        nlm_dirs.add(p.resolve())\n",
    "\n",
    "if not nlm_dirs:\n",
    "    raise FileNotFoundError(\n",
    "        \"No 'NLM_CLAHE' directories found under DATASET_PARENT. \"\n",
    "        \"Check folder names and capitalization.\"\n",
    "    )\n",
    "\n",
    "# --- collect BMP files from each NLM_CLAHE and map to class name (parent folder) ---\n",
    "rows = []\n",
    "seen_paths = set()   # dedupe absolute paths\n",
    "\n",
    "for nlm in sorted(nlm_dirs, key=lambda x: str(x)):\n",
    "    class_name = nlm.parent.name    # parent folder is the class label\n",
    "    # gather BMP files (case-insensitive)\n",
    "    bmp_files = [p.resolve() for p in nlm.iterdir() if p.is_file() and p.suffix.lower() == \".bmp\"]\n",
    "    if not bmp_files:\n",
    "        # warn but continue\n",
    "        print(f\"Warning: no .bmp files found in: {nlm}  (class = '{class_name}')\")\n",
    "        continue\n",
    "    for p in bmp_files:\n",
    "        sp = str(p)\n",
    "        if sp in seen_paths:\n",
    "            continue\n",
    "        seen_paths.add(sp)\n",
    "        rows.append((sp, class_name))\n",
    "\n",
    "# --- build DataFrame ---\n",
    "df = pd.DataFrame(rows, columns=[\"image_path\", \"label_name\"])\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"No .bmp image files were found in any discovered NLM_CLAHE directories.\")\n",
    "\n",
    "# stable sorted class ordering -> map to integer labels\n",
    "class_names = sorted(df[\"label_name\"].unique().tolist())\n",
    "label_to_id = {n: i for i, n in enumerate(class_names)}\n",
    "df[\"label\"] = df[\"label_name\"].map(label_to_id)\n",
    "\n",
    "# optional: shuffle rows (helps downstream splitting)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# summary prints\n",
    "print(\"Dataset parent:\", DATASET_PARENT)\n",
    "print(\"Discovered NLM_CLAHE directories (count):\", len(nlm_dirs))\n",
    "for p in sorted(nlm_dirs):\n",
    "    print(\"  -\", p)\n",
    "print(\"\\nFound classes (alphabetical):\", class_names)\n",
    "print(\"Total images found:\", len(df))\n",
    "print(\"Counts per class:\")\n",
    "print(df.groupby(\"label_name\").size().sort_values(ascending=False))\n",
    "\n",
    "# save csv to dataset parent for convenience\n",
    "csv_out = DATASET_PARENT / \"sipakmed_file_list.csv\"\n",
    "df.to_csv(csv_out, index=False)\n",
    "print(f\"\\nSaved file list to: {csv_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e38eb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3239\n",
      "Validation size: 405\n",
      "Test size: 405\n",
      "\n",
      "Train class counts:\n",
      " label_name\n",
      "im_Superficial-Intermediate    665\n",
      "im_Koilocytotic                660\n",
      "im_Dyskeratotic                650\n",
      "im_Metaplastic                 634\n",
      "im_Parabasal                   630\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation class counts:\n",
      " label_name\n",
      "im_Superficial-Intermediate    83\n",
      "im_Koilocytotic                83\n",
      "im_Dyskeratotic                81\n",
      "im_Metaplastic                 79\n",
      "im_Parabasal                   79\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test class counts:\n",
      " label_name\n",
      "im_Superficial-Intermediate    83\n",
      "im_Koilocytotic                82\n",
      "im_Dyskeratotic                82\n",
      "im_Metaplastic                 80\n",
      "im_Parabasal                   78\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# stratified split\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "# Optional: check class distribution\n",
    "print(\"\\nTrain class counts:\\n\", train_df['label_name'].value_counts())\n",
    "print(\"\\nValidation class counts:\\n\", val_df['label_name'].value_counts())\n",
    "print(\"\\nTest class counts:\\n\", test_df['label_name'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1deb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetDict, Features, ClassLabel, Image\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define features for HF dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m features \u001b[38;5;241m=\u001b[39m Features({\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: Image(),                \u001b[38;5;66;03m# image will be lazy-loaded\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: ClassLabel(names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()))\n\u001b[0;32m      7\u001b[0m })\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, Features, ClassLabel, Image\n",
    "\n",
    "# Define features for HF dataset\n",
    "features = Features({\n",
    "    \"image\": Image(),                # image will be lazy-loaded\n",
    "    \"label\": ClassLabel(names=sorted(df['label_name'].unique()))\n",
    "})\n",
    "\n",
    "def df_to_ds(dframe):\n",
    "    d = Dataset.from_dict({\n",
    "        \"image\": dframe[\"image_path\"].tolist(),\n",
    "        \"label\": dframe[\"label\"].tolist()\n",
    "    })\n",
    "    return d.cast(features)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": df_to_ds(train_df.reset_index(drop=True)),\n",
    "    \"validation\": df_to_ds(val_df.reset_index(drop=True)),\n",
    "    \"test\": df_to_ds(test_df.reset_index(drop=True))\n",
    "})\n",
    "\n",
    "# Quick check\n",
    "print(dataset)\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1891b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
