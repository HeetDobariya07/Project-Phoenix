{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb1a18b",
   "metadata": {},
   "source": [
    "# Preliminary Research ProtoPNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d2625",
   "metadata": {},
   "source": [
    "- ProtoPNet augments a normal CNN by storing a small batch of prototypes (like example feature matches). \n",
    "- For a new image, it finds which prototypes match part of the image, and final descision is made from those patches.\n",
    "- The model can show which training patch each prototype matches and where it matched on the test image, adding to the explaination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763c3e2",
   "metadata": {},
   "source": [
    "### Overall High Level Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input image\n",
    "  └→ Backbone CNN → Feature map (H × W × D)\n",
    "       └→ (optional) 1×1 adapter → Feature map (H × W × D')\n",
    "            └→ Prototype layer (M prototypes of size D'×p×p)\n",
    "                 └→ Per-prototype similarity scores (M values)\n",
    "                      └→ Linear classifier (weights W: num_classes × M)\n",
    "                           └→ Softmax → class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1eca60",
   "metadata": {},
   "source": [
    "### 1. Feature Extractor (Backbone CNN)\n",
    "- This part is just like a regular CNN similar to EfficientNetV2-S \n",
    "- It takes the image(256*256*3) and turns it into a feature map (a compressed version of the image where each number represents some feature like texture, edge, nucleus shape, etc)\n",
    "- It transforms the raw image into useful signals "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e78ab3",
   "metadata": {},
   "source": [
    "### 2. Prototype Layer\n",
    "- A prototype is a small patch in this feature map that represents a \"typical pattern\" for a class. Each prototype is learned during training\n",
    "- When the image passes through the CNN backbone, ProtoPNet checks if any patch in the image's feature map look similar to the stored prototypes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d772cf",
   "metadata": {},
   "source": [
    "### 3. Simialrity Computation\n",
    "- For each prototype, the model looks at every patch in the feature and asks how close it is to the prototype\n",
    "- Similarity is computed using distance like squared euclidian distance\n",
    "- It then picks the most similar patch via meethods like Global Max Pooling\n",
    "- Each prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b26d7",
   "metadata": {},
   "source": [
    "### 4.Fully Connected Layer(Classifier)\n",
    "- It is a linear layer that adds up evidences from the prototype layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd2fc3",
   "metadata": {},
   "source": [
    "### 5. Final Prediction\n",
    "- The network outputs probabilities for each class.\n",
    "- It also shows which prototypes were activated and where in the image they matched providing explainability "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c3ba2",
   "metadata": {},
   "source": [
    "## Training ProtoPNet\n",
    "\n",
    "ProtoPNet is trained in 3 main phases (don’t worry, simpler than it sounds):\n",
    "\n",
    "### 1. Joint Training:\n",
    "\n",
    "Train CNN backbone + prototypes together with normal classification loss.\n",
    "\n",
    "Also add two extra losses:\n",
    "\n",
    "- Clustering loss: prototypes should be close to patches of their own class.\n",
    "\n",
    "- Separation loss: prototypes should be far from patches of other classes.\n",
    "\n",
    "### 2. Projection Step:\n",
    "\n",
    "After some training, each prototype is “projected” onto the closest real patch from the training data.\n",
    "\n",
    "This makes prototypes real and interpretable (you can actually look at them as small image patches).\n",
    "\n",
    "### 3. Last-Layer Fine-Tuning:\n",
    "\n",
    "The final classifier weights are optimized so that prototypes contribute correctly to class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e3f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input Image (e.g., 224×224×3 cervical cell slide)\n",
    "        │\n",
    "        ▼\n",
    "────────────────────────────────────────────\n",
    "1. Backbone CNN (e.g., EfficientNetV2-S)\n",
    "────────────────────────────────────────────\n",
    "- Extracts features (edges, textures, nucleus shapes).\n",
    "- Output: Feature Map (e.g., 7×7×1280).\n",
    "        │\n",
    "        ▼\n",
    "────────────────────────────────────────────\n",
    "2. Prototype Layer\n",
    "────────────────────────────────────────────\n",
    "- Contains several \"prototypes\" (small patches in feature space).\n",
    "- Each prototype = typical pattern for a class.\n",
    "  Example: \"normal nucleus edge\", \"abnormal chromatin texture\".\n",
    "- For each prototype:\n",
    "    Compare with all patches in feature map.\n",
    "        │\n",
    "        ▼\n",
    "────────────────────────────────────────────\n",
    "3. Similarity Computation\n",
    "────────────────────────────────────────────\n",
    "- Compute similarity between prototype & feature patches.\n",
    "- Use distance (like L2 norm).\n",
    "- Global Max Pooling → pick the strongest match.\n",
    "- Output: Similarity scores for each prototype.\n",
    "        │\n",
    "        ▼\n",
    "────────────────────────────────────────────\n",
    "4. Fully Connected Layer (Linear Classifier)\n",
    "────────────────────────────────────────────\n",
    "- Combines similarity scores from prototypes.\n",
    "- Positive weights = evidence for a class.\n",
    "- Negative weights = evidence against a class.\n",
    "- Example:\n",
    "    Prototype #7 (abnormal nucleus) → strong for Dyskeratotic.\n",
    "    Prototype #2 (smooth edge) → strong for Normal.\n",
    "        │\n",
    "        ▼\n",
    "────────────────────────────────────────────\n",
    "5. Final Prediction\n",
    "────────────────────────────────────────────\n",
    "- Softmax layer → Probability distribution across classes.\n",
    "- Example: \n",
    "    Normal: 0.05\n",
    "    Parabasal: 0.10\n",
    "    Koilocytotic: 0.15\n",
    "    Dyskeratotic: 0.65\n",
    "    Metaplastic: 0.05\n",
    "        │\n",
    "        ▼\n",
    "────────────────────────────────────────────\n",
    "6. Explainability Output\n",
    "────────────────────────────────────────────\n",
    "- Shows which prototypes were activated.\n",
    "- Shows where in the input image the prototype matched.\n",
    "- Gives a \"this looks like that\" explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40900dd1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
