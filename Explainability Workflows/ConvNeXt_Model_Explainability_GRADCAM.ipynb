{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNeXt Model Explainability with GRAD-CAM\n",
    "\n",
    "## Overview\n",
    "This notebook provides comprehensive explainability analysis for a fine-tuned ConvNeXtV2 model on the SiPakMED cervical cancer cell dataset using:\n",
    "\n",
    "- **GRAD-CAM** (Gradient-weighted Class Activation Mapping)\n",
    "- **Guided GRAD-CAM** (High-resolution feature visualization)\n",
    "- **Layer-CAM** (Layer-wise activation mapping)\n",
    "- **Integrated Gradients** (Attribution method)\n",
    "- **Feature Visualization** (Understanding learned features)\n",
    "- **Inference with Visualization** (Real-time predictions with explanations)\n",
    "\n",
    "### Dataset Classes:\n",
    "1. Dyskeratotic\n",
    "2. Koilocytotic\n",
    "3. Metaplastic\n",
    "4. Parabasal\n",
    "5. Superficial-Intermediate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Install required packages for Google Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets torch torchvision\n",
    "!pip install -q grad-cam opencv-python-headless\n",
    "!pip install -q matplotlib seaborn scikit-learn\n",
    "!pip install -q pillow numpy pandas\n",
    "!pip install -q safetensors\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive\n",
    "\n",
    "Mount your Google Drive to access the trained model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    ConvNextV2ForImageClassification,\n",
    "    AutoImageProcessor,\n",
    "    ConvNextV2Config\n",
    ")\n",
    "\n",
    "# GRAD-CAM\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, LayerCAM, GuidedBackpropReLUModel\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration\n",
    "\n",
    "Set up paths to your model checkpoint directory in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "# Update this path to point to your model checkpoint folder in Google Drive\n",
    "MODEL_PATH = \"/content/drive/MyDrive/path/to/your/model/checkpoint\"\n",
    "\n",
    "# Class names (in the same order as your model's id2label)\n",
    "CLASS_NAMES = [\n",
    "    'im_Dyskeratotic',\n",
    "    'im_Koilocytotic',\n",
    "    'im_Metaplastic',\n",
    "    'im_Parabasal',\n",
    "    'im_Superficial-Intermediate'\n",
    "]\n",
    "\n",
    "# Display names (cleaner for visualization)\n",
    "DISPLAY_NAMES = [\n",
    "    'Dyskeratotic',\n",
    "    'Koilocytotic',\n",
    "    'Metaplastic',\n",
    "    'Parabasal',\n",
    "    'Superficial-Intermediate'\n",
    "]\n",
    "\n",
    "# Image preprocessing parameters\n",
    "IMG_SIZE = 224\n",
    "NORMALIZATION_MEAN = [0.485, 0.456, 0.406]\n",
    "NORMALIZATION_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Create id2label and label2id mappings\n",
    "id2label = {i: name for i, name in enumerate(CLASS_NAMES)}\n",
    "label2id = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
    "\n",
    "print(\"Configuration set up successfully!\")\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "print(f\"Number of classes: {len(CLASS_NAMES)}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model from Checkpoint\n",
    "\n",
    "Load the fine-tuned ConvNeXtV2 model from saved checkpoint files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path, device):\n",
    "    \"\"\"\n",
    "    Load ConvNeXtV2 model from checkpoint with all necessary files.\n",
    "    \n",
    "    Expected files in checkpoint_path:\n",
    "    - config.json\n",
    "    - model.safetensors\n",
    "    - preprocessor_config.json\n",
    "    - optimizer.pt (optional)\n",
    "    - scheduler.pt (optional)\n",
    "    - trainer_state.json (optional)\n",
    "    - training_args.bin (optional)\n",
    "    - rng_state.pth (optional)\n",
    "    - scaler.pt (optional)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOADING MODEL FROM CHECKPOINT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if checkpoint path exists\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint path not found: {checkpoint_path}\")\n",
    "    \n",
    "    # List files in checkpoint directory\n",
    "    checkpoint_files = os.listdir(checkpoint_path)\n",
    "    print(f\"\\nFiles found in checkpoint directory:\")\n",
    "    for file in sorted(checkpoint_files):\n",
    "        file_size = os.path.getsize(os.path.join(checkpoint_path, file)) / (1024 * 1024)\n",
    "        print(f\"  - {file} ({file_size:.2f} MB)\")\n",
    "    \n",
    "    # Load the model\n",
    "    print(\"\\nüì• Loading model...\")\n",
    "    try:\n",
    "        model = ConvNextV2ForImageClassification.from_pretrained(\n",
    "            checkpoint_path,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(\"‚úÖ Model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Load the image processor\n",
    "    print(\"\\nüì• Loading image processor...\")\n",
    "    try:\n",
    "        processor = AutoImageProcessor.from_pretrained(\n",
    "            checkpoint_path,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        print(\"‚úÖ Image processor loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading processor: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Print model information\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL INFORMATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Model type: {model.config.model_type}\")\n",
    "    print(f\"Number of labels: {model.config.num_labels}\")\n",
    "    print(f\"Hidden sizes: {model.config.hidden_sizes}\")\n",
    "    print(f\"Image size: {model.config.image_size}\")\n",
    "    \n",
    "    if hasattr(model.config, 'id2label'):\n",
    "        print(f\"\\nClass labels:\")\n",
    "        for idx, label in model.config.id2label.items():\n",
    "            print(f\"  {idx}: {label}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "# Load the model\n",
    "model, processor = load_model_from_checkpoint(MODEL_PATH, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, processor, return_original=True):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image for model input.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        processor: Image processor from the model\n",
    "        return_original: Whether to return the original image for visualization\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (preprocessed_tensor, original_image_array)\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Store original for visualization\n",
    "    original_image = np.array(image)\n",
    "    \n",
    "    # Preprocess using the model's processor\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs['pixel_values'].to(device)\n",
    "    \n",
    "    if return_original:\n",
    "        return pixel_values, original_image\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def denormalize_image(tensor, mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD):\n",
    "    \"\"\"\n",
    "    Denormalize a tensor image for visualization.\n",
    "    \n",
    "    Args:\n",
    "        tensor: Normalized image tensor (C, H, W)\n",
    "        mean: Normalization mean\n",
    "        std: Normalization standard deviation\n",
    "    \n",
    "    Returns:\n",
    "        Denormalized image as numpy array\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    \n",
    "    # Denormalize\n",
    "    denorm = tensor.cpu() * std + mean\n",
    "    denorm = denorm.clamp(0, 1)\n",
    "    \n",
    "    # Convert to numpy and transpose to (H, W, C)\n",
    "    return denorm.numpy().transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "def resize_image_for_cam(image, target_size=224):\n",
    "    \"\"\"\n",
    "    Resize image to match the CAM size.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image as numpy array\n",
    "        target_size: Target size for resizing\n",
    "    \n",
    "    Returns:\n",
    "        Resized image\n",
    "    \"\"\"\n",
    "    return cv2.resize(image, (target_size, target_size))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Image preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image_path, processor, display_names=DISPLAY_NAMES, top_k=3):\n",
    "    \"\"\"\n",
    "    Make prediction on an image and return results.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ConvNeXt model\n",
    "        image_path: Path to the image\n",
    "        processor: Image processor\n",
    "        display_names: List of class display names\n",
    "        top_k: Number of top predictions to return\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    pixel_values, original_image = load_and_preprocess_image(image_path, processor)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Get probabilities\n",
    "    probabilities = F.softmax(logits, dim=-1)[0]\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_probs, top_indices = torch.topk(probabilities, k=min(top_k, len(display_names)))\n",
    "    \n",
    "    # Get predicted class\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    predicted_class_name = display_names[predicted_class_idx]\n",
    "    predicted_confidence = probabilities[predicted_class_idx].item()\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'predicted_class': predicted_class_name,\n",
    "        'predicted_idx': predicted_class_idx,\n",
    "        'confidence': predicted_confidence,\n",
    "        'probabilities': probabilities.cpu().numpy(),\n",
    "        'top_k_classes': [display_names[idx] for idx in top_indices],\n",
    "        'top_k_probs': top_probs.cpu().numpy(),\n",
    "        'logits': logits.cpu().numpy()[0],\n",
    "        'original_image': original_image\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def display_prediction_results(results, figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    Display prediction results with visualization.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary from predict_image function\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Display image\n",
    "    axes[0].imshow(results['original_image'])\n",
    "    axes[0].set_title(f\"Predicted: {results['predicted_class']}\\nConfidence: {results['confidence']:.2%}\",\n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Display probability distribution\n",
    "    probabilities = results['probabilities']\n",
    "    class_names = DISPLAY_NAMES\n",
    "    \n",
    "    colors = ['#2ecc71' if i == results['predicted_idx'] else '#3498db' \n",
    "              for i in range(len(class_names))]\n",
    "    \n",
    "    axes[1].barh(class_names, probabilities * 100, color=colors)\n",
    "    axes[1].set_xlabel('Probability (%)', fontsize=11)\n",
    "    axes[1].set_title('Class Probabilities', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlim(0, 100)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        axes[1].text(prob * 100 + 1, i, f'{prob*100:.1f}%', \n",
    "                    va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PREDICTION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Predicted Class: {results['predicted_class']}\")\n",
    "    print(f\"Confidence: {results['confidence']:.2%}\")\n",
    "    print(f\"\\nTop {len(results['top_k_classes'])} Predictions:\")\n",
    "    for i, (cls, prob) in enumerate(zip(results['top_k_classes'], results['top_k_probs'])):\n",
    "        print(f\"  {i+1}. {cls}: {prob:.2%}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Inference functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. GRAD-CAM Implementation\n",
    "\n",
    "Gradient-weighted Class Activation Mapping (GRAD-CAM) visualizes which regions of the image are important for the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtGradCAMWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for ConvNeXtV2ForImageClassification to make it compatible with GRAD-CAM.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get the logits from the model\n",
    "        outputs = self.model(pixel_values=x)\n",
    "        return outputs.logits\n",
    "\n",
    "\n",
    "def get_target_layers(model):\n",
    "    \"\"\"\n",
    "    Get the target layers for GRAD-CAM from ConvNeXt model.\n",
    "    \n",
    "    For ConvNeXtV2, we typically use the last convolutional layer\n",
    "    from the last stage before the classification head.\n",
    "    \n",
    "    Args:\n",
    "        model: ConvNeXtV2ForImageClassification model\n",
    "    \n",
    "    Returns:\n",
    "        List of target layers for GRAD-CAM\n",
    "    \"\"\"\n",
    "    # ConvNeXtV2 structure: convnextv2.encoder.stages[-1]\n",
    "    # We want the last layer of the last stage\n",
    "    target_layers = [model.convnextv2.encoder.stages[-1].layers[-1]]\n",
    "    return target_layers\n",
    "\n",
    "\n",
    "def apply_gradcam(model, image_path, processor, target_class=None, \n",
    "                 use_cuda=True, colormap=cv2.COLORMAP_JET):\n",
    "    \"\"\"\n",
    "    Apply GRAD-CAM to visualize model attention.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ConvNeXt model\n",
    "        image_path: Path to the image\n",
    "        processor: Image processor\n",
    "        target_class: Target class index (None for predicted class)\n",
    "        use_cuda: Whether to use CUDA\n",
    "        colormap: OpenCV colormap for visualization\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with GRAD-CAM visualization and metadata\n",
    "    \"\"\"\n",
    "    # Wrap the model\n",
    "    wrapped_model = ConvNeXtGradCAMWrapper(model)\n",
    "    \n",
    "    # Get target layers\n",
    "    target_layers = get_target_layers(model)\n",
    "    \n",
    "    # Initialize GRAD-CAM\n",
    "    cam = GradCAM(model=wrapped_model, target_layers=target_layers, use_cuda=use_cuda)\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    pixel_values, original_image = load_and_preprocess_image(image_path, processor)\n",
    "    \n",
    "    # Get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = logits.argmax(-1).item()\n",
    "        probabilities = F.softmax(logits, dim=-1)[0]\n",
    "    \n",
    "    # Use predicted class if target not specified\n",
    "    if target_class is None:\n",
    "        target_class = predicted_class\n",
    "    \n",
    "    # Create target for GRAD-CAM\n",
    "    targets = [ClassifierOutputTarget(target_class)]\n",
    "    \n",
    "    # Generate GRAD-CAM\n",
    "    grayscale_cam = cam(input_tensor=pixel_values, targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    # Resize original image to match CAM size\n",
    "    resized_image = resize_image_for_cam(original_image, target_size=224)\n",
    "    \n",
    "    # Normalize image for visualization\n",
    "    rgb_image = resized_image.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Create visualization\n",
    "    visualization = show_cam_on_image(rgb_image, grayscale_cam, use_rgb=True, \n",
    "                                     colormap=colormap)\n",
    "    \n",
    "    return {\n",
    "        'original_image': original_image,\n",
    "        'resized_image': resized_image,\n",
    "        'grayscale_cam': grayscale_cam,\n",
    "        'visualization': visualization,\n",
    "        'predicted_class': predicted_class,\n",
    "        'target_class': target_class,\n",
    "        'confidence': probabilities[predicted_class].item(),\n",
    "        'all_probabilities': probabilities.cpu().numpy()\n",
    "    }\n",
    "\n",
    "\n",
    "def display_gradcam_results(results, display_names=DISPLAY_NAMES, figsize=(18, 6)):\n",
    "    \"\"\"\n",
    "    Display GRAD-CAM results in a comprehensive visualization.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary from apply_gradcam function\n",
    "        display_names: List of class display names\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(results['original_image'])\n",
    "    axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Heatmap only\n",
    "    axes[1].imshow(results['grayscale_cam'], cmap='jet')\n",
    "    axes[1].set_title('GRAD-CAM Heatmap', fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2].imshow(results['visualization'])\n",
    "    predicted_name = display_names[results['predicted_class']]\n",
    "    target_name = display_names[results['target_class']]\n",
    "    \n",
    "    title = f\"GRAD-CAM Overlay\\nPredicted: {predicted_name} ({results['confidence']:.2%})\"\n",
    "    if results['predicted_class'] != results['target_class']:\n",
    "        title += f\"\\nTarget: {target_name}\"\n",
    "    \n",
    "    axes[2].set_title(title, fontsize=14, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print information\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GRAD-CAM ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Predicted Class: {predicted_name}\")\n",
    "    print(f\"Confidence: {results['confidence']:.2%}\")\n",
    "    print(f\"Target Class for CAM: {target_name}\")\n",
    "    print(\"\\nHeatmap Interpretation:\")\n",
    "    print(\"  üî¥ Red regions: High importance for classification\")\n",
    "    print(\"  üîµ Blue regions: Low importance for classification\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(\"‚úÖ GRAD-CAM functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Layer-CAM Implementation\n",
    "\n",
    "Layer-CAM provides more fine-grained visualization by using layer activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_layercam(model, image_path, processor, target_class=None, \n",
    "                  use_cuda=True, colormap=cv2.COLORMAP_JET):\n",
    "    \"\"\"\n",
    "    Apply Layer-CAM to visualize model attention.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ConvNeXt model\n",
    "        image_path: Path to the image\n",
    "        processor: Image processor\n",
    "        target_class: Target class index (None for predicted class)\n",
    "        use_cuda: Whether to use CUDA\n",
    "        colormap: OpenCV colormap for visualization\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with Layer-CAM visualization and metadata\n",
    "    \"\"\"\n",
    "    # Wrap the model\n",
    "    wrapped_model = ConvNeXtGradCAMWrapper(model)\n",
    "    \n",
    "    # Get target layers\n",
    "    target_layers = get_target_layers(model)\n",
    "    \n",
    "    # Initialize Layer-CAM\n",
    "    cam = LayerCAM(model=wrapped_model, target_layers=target_layers, use_cuda=use_cuda)\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    pixel_values, original_image = load_and_preprocess_image(image_path, processor)\n",
    "    \n",
    "    # Get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = logits.argmax(-1).item()\n",
    "        probabilities = F.softmax(logits, dim=-1)[0]\n",
    "    \n",
    "    # Use predicted class if target not specified\n",
    "    if target_class is None:\n",
    "        target_class = predicted_class\n",
    "    \n",
    "    # Create target for Layer-CAM\n",
    "    targets = [ClassifierOutputTarget(target_class)]\n",
    "    \n",
    "    # Generate Layer-CAM\n",
    "    grayscale_cam = cam(input_tensor=pixel_values, targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    # Resize original image to match CAM size\n",
    "    resized_image = resize_image_for_cam(original_image, target_size=224)\n",
    "    \n",
    "    # Normalize image for visualization\n",
    "    rgb_image = resized_image.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Create visualization\n",
    "    visualization = show_cam_on_image(rgb_image, grayscale_cam, use_rgb=True, \n",
    "                                     colormap=colormap)\n",
    "    \n",
    "    return {\n",
    "        'original_image': original_image,\n",
    "        'resized_image': resized_image,\n",
    "        'grayscale_cam': grayscale_cam,\n",
    "        'visualization': visualization,\n",
    "        'predicted_class': predicted_class,\n",
    "        'target_class': target_class,\n",
    "        'confidence': probabilities[predicted_class].item(),\n",
    "        'all_probabilities': probabilities.cpu().numpy()\n",
    "    }\n",
    "\n",
    "\n",
    "def display_layercam_results(results, display_names=DISPLAY_NAMES, figsize=(18, 6)):\n",
    "    \"\"\"\n",
    "    Display Layer-CAM results in a comprehensive visualization.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary from apply_layercam function\n",
    "        display_names: List of class display names\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(results['original_image'])\n",
    "    axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Heatmap only\n",
    "    axes[1].imshow(results['grayscale_cam'], cmap='jet')\n",
    "    axes[1].set_title('Layer-CAM Heatmap', fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2].imshow(results['visualization'])\n",
    "    predicted_name = display_names[results['predicted_class']]\n",
    "    target_name = display_names[results['target_class']]\n",
    "    \n",
    "    title = f\"Layer-CAM Overlay\\nPredicted: {predicted_name} ({results['confidence']:.2%})\"\n",
    "    if results['predicted_class'] != results['target_class']:\n",
    "        title += f\"\\nTarget: {target_name}\"\n",
    "    \n",
    "    axes[2].set_title(title, fontsize=14, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LAYER-CAM ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Predicted Class: {predicted_name}\")\n",
    "    print(f\"Confidence: {results['confidence']:.2%}\")\n",
    "    print(f\"Target Class for CAM: {target_name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Layer-CAM functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. GRAD-CAM++ Implementation\n",
    "\n",
    "GRAD-CAM++ provides improved localization, especially for multiple instances of the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gradcam_plusplus(model, image_path, processor, target_class=None, \n",
    "                          use_cuda=True, colormap=cv2.COLORMAP_JET):\n",
    "    \"\"\"\n",
    "    Apply GRAD-CAM++ to visualize model attention.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ConvNeXt model\n",
    "        image_path: Path to the image\n",
    "        processor: Image processor\n",
    "        target_class: Target class index (None for predicted class)\n",
    "        use_cuda: Whether to use CUDA\n",
    "        colormap: OpenCV colormap for visualization\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with GRAD-CAM++ visualization and metadata\n",
    "    \"\"\"\n",
    "    # Wrap the model\n",
    "    wrapped_model = ConvNeXtGradCAMWrapper(model)\n",
    "    \n",
    "    # Get target layers\n",
    "    target_layers = get_target_layers(model)\n",
    "    \n",
    "    # Initialize GRAD-CAM++\n",
    "    cam = GradCAMPlusPlus(model=wrapped_model, target_layers=target_layers, use_cuda=use_cuda)\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    pixel_values, original_image = load_and_preprocess_image(image_path, processor)\n",
    "    \n",
    "    # Get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = logits.argmax(-1).item()\n",
    "        probabilities = F.softmax(logits, dim=-1)[0]\n",
    "    \n",
    "    # Use predicted class if target not specified\n",
    "    if target_class is None:\n",
    "        target_class = predicted_class\n",
    "    \n",
    "    # Create target for GRAD-CAM++\n",
    "    targets = [ClassifierOutputTarget(target_class)]\n",
    "    \n",
    "    # Generate GRAD-CAM++\n",
    "    grayscale_cam = cam(input_tensor=pixel_values, targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    # Resize original image to match CAM size\n",
    "    resized_image = resize_image_for_cam(original_image, target_size=224)\n",
    "    \n",
    "    # Normalize image for visualization\n",
    "    rgb_image = resized_image.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Create visualization\n",
    "    visualization = show_cam_on_image(rgb_image, grayscale_cam, use_rgb=True, \n",
    "                                     colormap=colormap)\n",
    "    \n",
    "    return {\n",
    "        'original_image': original_image,\n",
    "        'resized_image': resized_image,\n",
    "        'grayscale_cam': grayscale_cam,\n",
    "        'visualization': visualization,\n",
    "        'predicted_class': predicted_class,\n",
    "        'target_class': target_class,\n",
    "        'confidence': probabilities[predicted_class].item(),\n",
    "        'all_probabilities': probabilities.cpu().numpy()\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ GRAD-CAM++ functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Comparative Visualization\n",
    "\n",
    "Compare different explainability methods side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_explainability_methods(model, image_path, processor, target_class=None, \n",
    "                                  display_names=DISPLAY_NAMES, figsize=(20, 12)):\n",
    "    \"\"\"\n",
    "    Compare GRAD-CAM, GRAD-CAM++, and Layer-CAM side-by-side.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ConvNeXt model\n",
    "        image_path: Path to the image\n",
    "        processor: Image processor\n",
    "        target_class: Target class index (None for predicted class)\n",
    "        display_names: List of class display names\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    print(\"Generating visualizations...\")\n",
    "    \n",
    "    # Apply all methods\n",
    "    gradcam_results = apply_gradcam(model, image_path, processor, target_class)\n",
    "    gradcam_pp_results = apply_gradcam_plusplus(model, image_path, processor, target_class)\n",
    "    layercam_results = apply_layercam(model, image_path, processor, target_class)\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(3, 3, figsize=figsize)\n",
    "    \n",
    "    methods = ['GRAD-CAM', 'GRAD-CAM++', 'Layer-CAM']\n",
    "    results_list = [gradcam_results, gradcam_pp_results, layercam_results]\n",
    "    \n",
    "    for row, (method_name, results) in enumerate(zip(methods, results_list)):\n",
    "        # Original image\n",
    "        axes[row, 0].imshow(results['original_image'])\n",
    "        axes[row, 0].set_title(f'{method_name}\\nOriginal Image', fontsize=12, fontweight='bold')\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        # Heatmap\n",
    "        axes[row, 1].imshow(results['grayscale_cam'], cmap='jet')\n",
    "        axes[row, 1].set_title(f'{method_name}\\nHeatmap', fontsize=12, fontweight='bold')\n",
    "        axes[row, 1].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        axes[row, 2].imshow(results['visualization'])\n",
    "        predicted_name = display_names[results['predicted_class']]\n",
    "        axes[row, 2].set_title(f'{method_name}\\nOverlay: {predicted_name}', \n",
    "                              fontsize=12, fontweight='bold')\n",
    "        axes[row, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Comparison of Explainability Methods\\nPredicted: {predicted_name} (Confidence: {results['confidence']:.2%})\",\n",
    "                fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comparison summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPLAINABILITY METHODS COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Image: {os.path.basename(image_path)}\")\n",
    "    print(f\"Predicted Class: {predicted_name}\")\n",
    "    print(f\"Confidence: {gradcam_results['confidence']:.2%}\")\n",
    "    print(\"\\nMethod Characteristics:\")\n",
    "    print(\"  ‚Ä¢ GRAD-CAM: Classic gradient-based attention (good general performance)\")\n",
    "    print(\"  ‚Ä¢ GRAD-CAM++: Improved localization for multiple instances\")\n",
    "    print(\"  ‚Ä¢ Layer-CAM: Fine-grained layer-wise attention\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Comparative visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Multi-Class Explainability\n",
    "\n",
    "Visualize what the model focuses on for each class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_classes(model, image_path, processor, display_names=DISPLAY_NAMES, \n",
    "                         method='gradcam', figsize=(20, 8)):\n",
    "    \"\"\"\n",
    "    Visualize GRAD-CAM for all classes to understand what the model looks for\n",
    "    when predicting each class.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ConvNeXt model\n",
    "        image_path: Path to the image\n",
    "        processor: Image processor\n",
    "        display_names: List of class display names\n",
    "        method: Which CAM method to use ('gradcam', 'gradcam++', 'layercam')\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    # Select the appropriate method\n",
    "    if method == 'gradcam':\n",
    "        apply_func = apply_gradcam\n",
    "        title_prefix = \"GRAD-CAM\"\n",
    "    elif method == 'gradcam++':\n",
    "        apply_func = apply_gradcam_plusplus\n",
    "        title_prefix = \"GRAD-CAM++\"\n",
    "    else:\n",
    "        apply_func = apply_layercam\n",
    "        title_prefix = \"Layer-CAM\"\n",
    "    \n",
    "    print(f\"Generating {title_prefix} for all classes...\")\n",
    "    \n",
    "    # Get prediction first\n",
    "    pixel_values, original_image = load_and_preprocess_image(image_path, processor)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = logits.argmax(-1).item()\n",
    "        probabilities = F.softmax(logits, dim=-1)[0]\n",
    "    \n",
    "    # Create figure\n",
    "    num_classes = len(display_names)\n",
    "    fig, axes = plt.subplots(2, num_classes, figsize=figsize)\n",
    "    \n",
    "    if num_classes == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    # Generate CAM for each class\n",
    "    for class_idx in range(num_classes):\n",
    "        results = apply_func(model, image_path, processor, target_class=class_idx)\n",
    "        \n",
    "        # Heatmap\n",
    "        axes[0, class_idx].imshow(results['grayscale_cam'], cmap='jet')\n",
    "        is_predicted = (class_idx == predicted_class)\n",
    "        border_color = 'green' if is_predicted else 'gray'\n",
    "        \n",
    "        title = f\"{display_names[class_idx]}\\n{probabilities[class_idx]:.1%}\"\n",
    "        if is_predicted:\n",
    "            title = \"‚úì \" + title\n",
    "        \n",
    "        axes[0, class_idx].set_title(title, fontsize=11, \n",
    "                                     fontweight='bold' if is_predicted else 'normal',\n",
    "                                     color='green' if is_predicted else 'black')\n",
    "        axes[0, class_idx].axis('off')\n",
    "        \n",
    "        # Add border to predicted class\n",
    "        if is_predicted:\n",
    "            for spine in axes[0, class_idx].spines.values():\n",
    "                spine.set_edgecolor(border_color)\n",
    "                spine.set_linewidth(3)\n",
    "        \n",
    "        # Overlay\n",
    "        axes[1, class_idx].imshow(results['visualization'])\n",
    "        axes[1, class_idx].axis('off')\n",
    "        \n",
    "        if is_predicted:\n",
    "            for spine in axes[1, class_idx].spines.values():\n",
    "                spine.set_edgecolor(border_color)\n",
    "                spine.set_linewidth(3)\n",
    "    \n",
    "    plt.suptitle(f\"{title_prefix} Analysis for All Classes\\nPredicted: {display_names[predicted_class]} ({probabilities[predicted_class]:.2%})\",\n",
    "                fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MULTI-CLASS EXPLAINABILITY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Predicted Class: {display_names[predicted_class]} ({probabilities[predicted_class]:.2%})\")\n",
    "    print(\"\\nClass Probabilities:\")\n",
    "    for idx, (name, prob) in enumerate(zip(display_names, probabilities)):\n",
    "        marker = \"‚úì\" if idx == predicted_class else \" \"\n",
    "        print(f\"  {marker} {name}: {prob:.2%}\")\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"  The visualizations show which regions the model focuses on for each class.\")\n",
    "    print(\"  Green border indicates the predicted class.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Multi-class explainability functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Example Usage: Test Image\n",
    "\n",
    "Upload an image and run explainability analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload an image from your computer\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "print(\"Please upload a cervical cell image (.bmp, .png, .jpg):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    # Get the first uploaded file\n",
    "    image_name = list(uploaded.keys())[0]\n",
    "    image_path = f\"/content/{image_name}\"\n",
    "    \n",
    "    # Save the uploaded file\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(uploaded[image_name])\n",
    "    \n",
    "    print(f\"\\n‚úÖ Image uploaded: {image_name}\")\n",
    "    print(f\"Image saved to: {image_path}\")\n",
    "else:\n",
    "    # Use a default path if no file uploaded\n",
    "    image_path = \"/content/drive/MyDrive/path/to/test/image.bmp\"\n",
    "    print(f\"Using default image path: {image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1 Basic Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction and display results\n",
    "results = predict_image(model, image_path, processor, display_names=DISPLAY_NAMES)\n",
    "display_prediction_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2 GRAD-CAM Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply GRAD-CAM\n",
    "gradcam_results = apply_gradcam(model, image_path, processor, use_cuda=torch.cuda.is_available())\n",
    "display_gradcam_results(gradcam_results, display_names=DISPLAY_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3 Layer-CAM Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Layer-CAM\n",
    "layercam_results = apply_layercam(model, image_path, processor, use_cuda=torch.cuda.is_available())\n",
    "display_layercam_results(layercam_results, display_names=DISPLAY_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4 Compare All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all explainability methods\n",
    "compare_explainability_methods(model, image_path, processor, display_names=DISPLAY_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5 Multi-Class Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what the model looks for in each class\n",
    "visualize_all_classes(model, image_path, processor, display_names=DISPLAY_NAMES, method='gradcam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Batch Analysis\n",
    "\n",
    "Analyze multiple images from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_analyze_images(model, image_dir, processor, display_names=DISPLAY_NAMES, \n",
    "                        method='gradcam', max_images=10):\n",
    "    \"\"\"\n",
    "    Analyze multiple images from a directory.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ConvNeXt model\n",
    "        image_dir: Directory containing images\n",
    "        processor: Image processor\n",
    "        display_names: List of class display names\n",
    "        method: CAM method to use\n",
    "        max_images: Maximum number of images to process\n",
    "    \"\"\"\n",
    "    # Get image files\n",
    "    image_extensions = ['.bmp', '.png', '.jpg', '.jpeg']\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(Path(image_dir).glob(f\"*{ext}\"))\n",
    "        image_files.extend(Path(image_dir).glob(f\"*{ext.upper()}\"))\n",
    "    \n",
    "    image_files = sorted(list(set(image_files)))[:max_images]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {image_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images. Processing...\\n\")\n",
    "    \n",
    "    # Select method\n",
    "    if method == 'gradcam':\n",
    "        apply_func = apply_gradcam\n",
    "    elif method == 'gradcam++':\n",
    "        apply_func = apply_gradcam_plusplus\n",
    "    else:\n",
    "        apply_func = apply_layercam\n",
    "    \n",
    "    # Process each image\n",
    "    results_summary = []\n",
    "    \n",
    "    for idx, image_path in enumerate(image_files):\n",
    "        print(f\"Processing {idx+1}/{len(image_files)}: {image_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Get prediction\n",
    "            pred_results = predict_image(model, str(image_path), processor, display_names)\n",
    "            \n",
    "            # Apply CAM\n",
    "            cam_results = apply_func(model, str(image_path), processor)\n",
    "            \n",
    "            # Store results\n",
    "            results_summary.append({\n",
    "                'image_name': image_path.name,\n",
    "                'predicted_class': pred_results['predicted_class'],\n",
    "                'confidence': pred_results['confidence'],\n",
    "                'top_3_classes': ', '.join(pred_results['top_k_classes'][:3])\n",
    "            })\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"  Predicted: {pred_results['predicted_class']} ({pred_results['confidence']:.2%})\\n\")\n",
    "            \n",
    "            # Visualize\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            fig.suptitle(f\"{image_path.name}\\n{pred_results['predicted_class']} ({pred_results['confidence']:.2%})\",\n",
    "                        fontsize=14, fontweight='bold')\n",
    "            \n",
    "            axes[0].imshow(cam_results['original_image'])\n",
    "            axes[0].set_title('Original')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(cam_results['grayscale_cam'], cmap='jet')\n",
    "            axes[1].set_title('Heatmap')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            axes[2].imshow(cam_results['visualization'])\n",
    "            axes[2].set_title('Overlay')\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {image_path.name}: {str(e)}\\n\")\n",
    "            continue\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    if results_summary:\n",
    "        df_summary = pd.DataFrame(results_summary)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BATCH ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(df_summary.to_string(index=False))\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return df_summary\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"‚úÖ Batch analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Batch Analysis (Uncomment to use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify the path below to analyze multiple images\n",
    "# batch_image_dir = \"/content/drive/MyDrive/path/to/image/directory\"\n",
    "# df_results = batch_analyze_images(model, batch_image_dir, processor, \n",
    "#                                   display_names=DISPLAY_NAMES, method='gradcam', max_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Results\n",
    "\n",
    "Save visualizations and results for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_explainability_results(model, image_path, processor, output_dir, \n",
    "                               display_names=DISPLAY_NAMES):\n",
    "    \"\"\"\n",
    "    Generate and save all explainability visualizations.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ConvNeXt model\n",
    "        image_path: Path to the image\n",
    "        processor: Image processor\n",
    "        output_dir: Directory to save results\n",
    "        display_names: List of class display names\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    image_name = Path(image_path).stem\n",
    "    \n",
    "    print(f\"Saving explainability results for {image_name}...\")\n",
    "    \n",
    "    # Get prediction\n",
    "    pred_results = predict_image(model, image_path, processor, display_names)\n",
    "    \n",
    "    # Save original image\n",
    "    original_save_path = os.path.join(output_dir, f\"{image_name}_original.png\")\n",
    "    Image.fromarray(pred_results['original_image']).save(original_save_path)\n",
    "    print(f\"‚úÖ Saved: {original_save_path}\")\n",
    "    \n",
    "    # Generate and save GRAD-CAM\n",
    "    gradcam_results = apply_gradcam(model, image_path, processor)\n",
    "    gradcam_save_path = os.path.join(output_dir, f\"{image_name}_gradcam.png\")\n",
    "    Image.fromarray(gradcam_results['visualization']).save(gradcam_save_path)\n",
    "    print(f\"‚úÖ Saved: {gradcam_save_path}\")\n",
    "    \n",
    "    # Generate and save Layer-CAM\n",
    "    layercam_results = apply_layercam(model, image_path, processor)\n",
    "    layercam_save_path = os.path.join(output_dir, f\"{image_name}_layercam.png\")\n",
    "    Image.fromarray(layercam_results['visualization']).save(layercam_save_path)\n",
    "    print(f\"‚úÖ Saved: {layercam_save_path}\")\n",
    "    \n",
    "    # Save prediction results as JSON\n",
    "    import json\n",
    "    results_dict = {\n",
    "        'image_name': image_name,\n",
    "        'predicted_class': pred_results['predicted_class'],\n",
    "        'confidence': float(pred_results['confidence']),\n",
    "        'top_3_predictions': [\n",
    "            {'class': cls, 'probability': float(prob)}\n",
    "            for cls, prob in zip(pred_results['top_k_classes'][:3], pred_results['top_k_probs'][:3])\n",
    "        ],\n",
    "        'all_probabilities': {name: float(prob) for name, prob in zip(display_names, pred_results['probabilities'])}\n",
    "    }\n",
    "    \n",
    "    json_save_path = os.path.join(output_dir, f\"{image_name}_results.json\")\n",
    "    with open(json_save_path, 'w') as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "    print(f\"‚úÖ Saved: {json_save_path}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ All results saved to: {output_dir}\")\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "print(\"‚úÖ Save results functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Save Results (Uncomment to use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save results\n",
    "# output_directory = \"/content/drive/MyDrive/explainability_results\"\n",
    "# saved_results = save_explainability_results(model, image_path, processor, output_directory, \n",
    "#                                            display_names=DISPLAY_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Summary and Conclusions\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **GRAD-CAM**: Provides gradient-based attention maps showing which regions influence the model's decision\n",
    "2. **GRAD-CAM++**: Improved localization, especially useful for multiple instances\n",
    "3. **Layer-CAM**: Layer-wise activation mapping for fine-grained analysis\n",
    "4. **Multi-Class Analysis**: Understand what features the model associates with each class\n",
    "\n",
    "### Usage Tips:\n",
    "\n",
    "- Use **GRAD-CAM** for general explainability analysis\n",
    "- Use **Layer-CAM** for more detailed, fine-grained visualizations\n",
    "- Compare multiple methods to get a comprehensive understanding\n",
    "- Analyze multiple images to identify patterns in model behavior\n",
    "\n",
    "### Medical Interpretation:\n",
    "\n",
    "When analyzing cervical cell images:\n",
    "- Red/hot regions indicate areas the model focuses on for classification\n",
    "- Different cell types may show attention to different morphological features\n",
    "- Compare attention maps across classes to understand distinctive features\n",
    "- Validate that the model focuses on clinically relevant regions\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This notebook is designed for Google Colab. Adjust paths and configurations as needed for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
