{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cervical Cancer Cell Classification Pipeline\n",
    "## Project Phoenix - Herlev Dataset Analysis\n",
    "\n",
    "This notebook implements a complete classification pipeline:\n",
    "1. Data Loading and Preparation\n",
    "2. Baseline Model Evaluation (VGG, DenseNet, ResNet)\n",
    "3. Hybrid Model Implementation (ResNet50 + Logistic Classifier)\n",
    "4. Final Evaluation and Comparison\n",
    "5. SHAP Explainability\n",
    "6. Streamlit Dashboard Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch torchvision scikit-learn shap streamlit matplotlib seaborn pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_PATH = \"/content/drive/MyDrive/Projects/6_Project Phoenix_Cervical Cancer Cell Classification/Herlev Dataset/Preprocessing Analysis v3.0\"\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, \"train\")\n",
    "TEST_PATH = os.path.join(DATA_PATH, \"test\")\n",
    "\n",
    "# Model configuration\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "# Class names (Herlev dataset 7 classes)\n",
    "CLASS_NAMES = [\n",
    "    'carcinoma_in_situ',\n",
    "    'light_dysplastic',\n",
    "    'moderate_dysplastic',\n",
    "    'normal_columnar',\n",
    "    'normal_intermediate',\n",
    "    'normal_superficial',\n",
    "    'severe_dysplastic'\n",
    "]\n",
    "\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HerlevDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Herlev cervical cell images.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(sorted(os.listdir(root_dir)))}\n",
    "        \n",
    "        # Load all image paths and labels\n",
    "        for class_name in os.listdir(root_dir):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for img_name in os.listdir(class_path):\n",
    "                    if img_name.lower().endswith(('.bmp', '.png', '.jpg', '.jpeg')):\n",
    "                        self.images.append(os.path.join(class_path, img_name))\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images from {root_dir}\")\n",
    "        print(f\"Class mapping: {self.class_to_idx}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HerlevDataset(TRAIN_PATH, transform=train_transform)\n",
    "test_dataset = HerlevDataset(TEST_PATH, transform=test_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "def visualize_samples(dataset, num_samples=10):\n",
    "    \"\"\"Display sample images from the dataset.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Get class names from dataset\n",
    "    idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "    \n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img, label = dataset[idx]\n",
    "        # Denormalize for visualization\n",
    "        img = img.numpy().transpose(1, 2, 0)\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(idx_to_class[label], fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Images from Dataset', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution visualization\n",
    "def plot_class_distribution(dataset, title=\"Class Distribution\"):\n",
    "    \"\"\"Plot the distribution of classes in the dataset.\"\"\"\n",
    "    idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "    class_counts = {}\n",
    "    \n",
    "    for label in dataset.labels:\n",
    "        class_name = idx_to_class[label]\n",
    "        class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.bar(class_counts.keys(), class_counts.values(), color='steelblue')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "train_distribution = plot_class_distribution(train_dataset, \"Training Set Class Distribution\")\n",
    "test_distribution = plot_class_distribution(test_dataset, \"Test Set Class Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model Evaluation\n",
    "\n",
    "We'll evaluate multiple pre-trained CNN architectures:\n",
    "- VGG16\n",
    "- DenseNet121\n",
    "- ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, num_classes, pretrained=True):\n",
    "    \"\"\"Create a pre-trained model with modified classifier for our task.\"\"\"\n",
    "    \n",
    "    if model_name == 'vgg16':\n",
    "        model = models.vgg16(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "        model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "        \n",
    "    elif model_name == 'densenet121':\n",
    "        model = models.densenet121(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "        \n",
    "    elif model_name == 'resnet50':\n",
    "        model = models.resnet50(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs, learning_rate):\n",
    "    \"\"\"Train a model and return training history.\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = train_loss / train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_loss = test_loss / test_total\n",
    "        test_acc = test_correct / test_total\n",
    "        \n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
    "              f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}\")\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "    \n",
    "    return history, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate model and return predictions and true labels.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot training and validation curves.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(history['test_loss'], label='Test Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'{model_name} - Loss Curves')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(history['train_acc'], label='Train Accuracy')\n",
    "    ax2.plot(history['test_acc'], label='Test Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title(f'{model_name} - Accuracy Curves')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names, title):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate baseline models\n",
    "baseline_models = ['vgg16', 'densenet121', 'resnet50']\n",
    "results = {}\n",
    "\n",
    "for model_name in baseline_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_model(model_name, NUM_CLASSES)\n",
    "    history, best_acc = train_model(model, train_loader, test_loader, NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate\n",
    "    preds, labels = evaluate_model(model, test_loader)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'best_acc': best_acc,\n",
    "        'predictions': preds,\n",
    "        'labels': labels\n",
    "    }\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history, model_name)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(classification_report(labels, preds, target_names=list(train_dataset.class_to_idx.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_data = []\n",
    "for model_name, result in results.items():\n",
    "    acc = accuracy_score(result['labels'], result['predictions'])\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        result['labels'], result['predictions'], average='weighted'\n",
    "    )\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': f\"{acc:.4f}\",\n",
    "        'Precision': f\"{precision:.4f}\",\n",
    "        'Recall': f\"{recall:.4f}\",\n",
    "        'F1-Score': f\"{f1:.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['best_acc'])\n",
    "print(f\"\\nBest performing model: {best_model_name.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hybrid Model Implementation\n",
    "\n",
    "Using ResNet50 as a fixed feature extractor + Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"ResNet50 feature extractor (frozen weights).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet50(weights='IMAGENET1K_V1')\n",
    "        # Remove the final classification layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        # Freeze all layers\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return x\n",
    "\n",
    "# Create feature extractor\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "feature_extractor.eval()\n",
    "\n",
    "print(\"Feature extractor created (ResNet50 with frozen weights)\")\n",
    "print(f\"Output feature dimension: 2048\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader):\n",
    "    \"\"\"Extract features from all images in a dataloader.\"\"\"\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            images = images.to(device)\n",
    "            features = model(images)\n",
    "            features_list.append(features.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "    \n",
    "    return np.vstack(features_list), np.concatenate(labels_list)\n",
    "\n",
    "# Extract features\n",
    "print(\"Extracting training features...\")\n",
    "X_train, y_train = extract_features(feature_extractor, train_loader)\n",
    "\n",
    "print(\"Extracting test features...\")\n",
    "X_test, y_test = extract_features(feature_extractor, test_loader)\n",
    "\n",
    "print(f\"\\nTraining features shape: {X_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression classifier\n",
    "print(\"Training Logistic Regression classifier...\")\n",
    "\n",
    "logistic_clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    random_state=SEED,\n",
    "    C=1.0\n",
    ")\n",
    "\n",
    "logistic_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "hybrid_train_preds = logistic_clf.predict(X_train_scaled)\n",
    "hybrid_test_preds = logistic_clf.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "hybrid_train_acc = accuracy_score(y_train, hybrid_train_preds)\n",
    "hybrid_test_acc = accuracy_score(y_test, hybrid_test_preds)\n",
    "\n",
    "print(f\"\\nHybrid Model (ResNet50 + Logistic Regression) Results:\")\n",
    "print(f\"Training Accuracy: {hybrid_train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {hybrid_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of hybrid model\n",
    "print(\"\\nHybrid Model Classification Report:\")\n",
    "print(classification_report(y_test, hybrid_test_preds, \n",
    "                           target_names=list(train_dataset.class_to_idx.keys())))\n",
    "\n",
    "# Confusion matrix\n",
    "plot_confusion_matrix(y_test, hybrid_test_preds, \n",
    "                     list(train_dataset.class_to_idx.keys()),\n",
    "                     \"Hybrid Model Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison including hybrid model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_comparison = []\n",
    "\n",
    "# Add baseline results\n",
    "for model_name, result in results.items():\n",
    "    acc = accuracy_score(result['labels'], result['predictions'])\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        result['labels'], result['predictions'], average='weighted'\n",
    "    )\n",
    "    final_comparison.append({\n",
    "        'Model': f\"{model_name} (Baseline)\",\n",
    "        'Accuracy': acc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "\n",
    "# Add hybrid model results\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_test, hybrid_test_preds, average='weighted'\n",
    ")\n",
    "final_comparison.append({\n",
    "    'Model': 'ResNet50 + LogReg (Hybrid)',\n",
    "    'Accuracy': hybrid_test_acc,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1\n",
    "})\n",
    "\n",
    "# Create and display comparison dataframe\n",
    "final_df = pd.DataFrame(final_comparison)\n",
    "final_df = final_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# Format for display\n",
    "display_df = final_df.copy()\n",
    "for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
    "    display_df[col] = display_df[col].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of model comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(final_df))\n",
    "width = 0.2\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    values = final_df[metric].values\n",
    "    ax.bar(x + i*width, values, width, label=metric, color=color)\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(final_df['Model'].values, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SHAP Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SHAP if needed\n",
    "# !pip install shap\n",
    "\n",
    "import shap\n",
    "\n",
    "print(\"Setting up SHAP explainer...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer for the logistic regression model\n",
    "# Using a subset of training data as background\n",
    "background_size = min(100, len(X_train_scaled))\n",
    "background = X_train_scaled[np.random.choice(len(X_train_scaled), background_size, replace=False)]\n",
    "\n",
    "# Create explainer\n",
    "explainer = shap.LinearExplainer(logistic_clf, background)\n",
    "\n",
    "# Calculate SHAP values for test set (using a subset for speed)\n",
    "test_subset_size = min(50, len(X_test_scaled))\n",
    "test_subset_idx = np.random.choice(len(X_test_scaled), test_subset_size, replace=False)\n",
    "X_test_subset = X_test_scaled[test_subset_idx]\n",
    "\n",
    "print(f\"Calculating SHAP values for {test_subset_size} test samples...\")\n",
    "shap_values = explainer.shap_values(X_test_subset)\n",
    "\n",
    "print(\"SHAP values calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot - Feature importance across all classes\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_subset, \n",
    "                  feature_names=[f\"Feature_{i}\" for i in range(X_test_subset.shape[1])],\n",
    "                  class_names=list(train_dataset.class_to_idx.keys()),\n",
    "                  show=False)\n",
    "plt.tight_layout()\n",
    "plt.title(\"SHAP Feature Importance Summary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for mean absolute SHAP values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Calculate mean absolute SHAP values across all classes\n",
    "if isinstance(shap_values, list):\n",
    "    mean_shap = np.mean([np.abs(sv).mean(axis=0) for sv in shap_values], axis=0)\n",
    "else:\n",
    "    mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Get top 20 most important features\n",
    "top_k = 20\n",
    "top_indices = np.argsort(mean_shap)[-top_k:]\n",
    "\n",
    "plt.barh(range(top_k), mean_shap[top_indices], color='steelblue')\n",
    "plt.yticks(range(top_k), [f\"Feature_{i}\" for i in top_indices])\n",
    "plt.xlabel('Mean |SHAP value|')\n",
    "plt.title(f'Top {top_k} Most Important Features (ResNet50 Features)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual prediction explanation\n",
    "def explain_prediction(idx, X_test_subset, y_test_subset, shap_values, class_names):\n",
    "    \"\"\"Explain a single prediction using SHAP.\"\"\"\n",
    "    \n",
    "    true_label = y_test_subset[idx]\n",
    "    pred_label = logistic_clf.predict(X_test_subset[idx:idx+1])[0]\n",
    "    \n",
    "    print(f\"Sample {idx}:\")\n",
    "    print(f\"  True class: {class_names[true_label]}\")\n",
    "    print(f\"  Predicted class: {class_names[pred_label]}\")\n",
    "    print(f\"  Correct: {true_label == pred_label}\")\n",
    "    \n",
    "    # Force plot for predicted class\n",
    "    if isinstance(shap_values, list):\n",
    "        sv = shap_values[pred_label][idx]\n",
    "    else:\n",
    "        sv = shap_values[idx]\n",
    "    \n",
    "    shap.force_plot(explainer.expected_value[pred_label] if isinstance(explainer.expected_value, np.ndarray) else explainer.expected_value,\n",
    "                   sv, X_test_subset[idx],\n",
    "                   matplotlib=True)\n",
    "\n",
    "# Explain a few predictions\n",
    "y_test_subset = y_test[test_subset_idx]\n",
    "class_names = list(train_dataset.class_to_idx.keys())\n",
    "\n",
    "for i in range(3):\n",
    "    explain_prediction(i, X_test_subset, y_test_subset, shap_values, class_names)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Create models directory\n",
    "MODELS_DIR = \"./saved_models\"\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Save feature extractor\n",
    "torch.save(feature_extractor.state_dict(), os.path.join(MODELS_DIR, \"resnet50_feature_extractor.pth\"))\n",
    "\n",
    "# Save logistic regression model and scaler\n",
    "with open(os.path.join(MODELS_DIR, \"logistic_classifier.pkl\"), 'wb') as f:\n",
    "    pickle.dump(logistic_clf, f)\n",
    "\n",
    "with open(os.path.join(MODELS_DIR, \"scaler.pkl\"), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save class mapping\n",
    "with open(os.path.join(MODELS_DIR, \"class_mapping.pkl\"), 'wb') as f:\n",
    "    pickle.dump(train_dataset.class_to_idx, f)\n",
    "\n",
    "# Save results\n",
    "final_df.to_csv(os.path.join(MODELS_DIR, \"model_comparison_results.csv\"), index=False)\n",
    "\n",
    "print(f\"Models and results saved to {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Streamlit Dashboard\n",
    "\n",
    "Run the following code to create a Streamlit dashboard file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit_code = '''\n",
    "\"\"\"\n",
    "Cervical Cancer Cell Classification Dashboard\n",
    "Project Phoenix - Streamlit Application\n",
    "\n",
    "Run with: streamlit run streamlit_app.py\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Project Phoenix - Cell Classification\",\n",
    "    page_icon=\"üî¨\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 224\n",
    "MODELS_DIR = \"./saved_models\"\n",
    "\n",
    "# Load models\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    \"\"\"Load the feature extractor and classifier.\"\"\"\n",
    "    \n",
    "    # Feature extractor\n",
    "    class FeatureExtractor(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(FeatureExtractor, self).__init__()\n",
    "            resnet = models.resnet50(weights=None)\n",
    "            self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return x\n",
    "    \n",
    "    device = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\n",
    "    \n",
    "    feature_extractor = FeatureExtractor()\n",
    "    feature_extractor.load_state_dict(torch.load(\n",
    "        os.path.join(MODELS_DIR, \"resnet50_feature_extractor.pth\"),\n",
    "        map_location=device\n",
    "    ))\n",
    "    feature_extractor.eval()\n",
    "    feature_extractor.to(device)\n",
    "    \n",
    "    # Classifier and scaler\n",
    "    with open(os.path.join(MODELS_DIR, \"logistic_classifier.pkl\"), \\'rb\\') as f:\n",
    "        classifier = pickle.load(f)\n",
    "    \n",
    "    with open(os.path.join(MODELS_DIR, \"scaler.pkl\"), \\'rb\\') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    with open(os.path.join(MODELS_DIR, \"class_mapping.pkl\"), \\'rb\\') as f:\n",
    "        class_mapping = pickle.load(f)\n",
    "    \n",
    "    return feature_extractor, classifier, scaler, class_mapping, device\n",
    "\n",
    "# Image preprocessing\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Preprocess image for model input.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "# Prediction function\n",
    "def predict(image, feature_extractor, classifier, scaler, class_mapping, device):\n",
    "    \"\"\"Make prediction on an image.\"\"\"\n",
    "    \n",
    "    # Preprocess\n",
    "    img_tensor = preprocess_image(image).to(device)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(img_tensor).cpu().numpy()\n",
    "    \n",
    "    # Scale and predict\n",
    "    features_scaled = scaler.transform(features)\n",
    "    prediction = classifier.predict(features_scaled)[0]\n",
    "    probabilities = classifier.predict_proba(features_scaled)[0]\n",
    "    \n",
    "    # Get class names\n",
    "    idx_to_class = {v: k for k, v in class_mapping.items()}\n",
    "    \n",
    "    return idx_to_class[prediction], probabilities, idx_to_class\n",
    "\n",
    "# Main app\n",
    "def main():\n",
    "    st.title(\"üî¨ Project Phoenix\")\n",
    "    st.subheader(\"Cervical Cancer Cell Classification\")\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Sidebar\n",
    "    st.sidebar.title(\"About\")\n",
    "    st.sidebar.info(\n",
    "        \"This application uses a hybrid deep learning model \"\n",
    "        \"(ResNet50 + Logistic Regression) to classify cervical cells \"\n",
    "        \"into 7 categories based on the Herlev dataset.\"\n",
    "    )\n",
    "    \n",
    "    st.sidebar.title(\"Cell Classes\")\n",
    "    st.sidebar.markdown(\"\"\"\n",
    "    - Carcinoma in situ\n",
    "    - Light dysplastic\n",
    "    - Moderate dysplastic\n",
    "    - Normal columnar\n",
    "    - Normal intermediate\n",
    "    - Normal superficial\n",
    "    - Severe dysplastic\n",
    "    \"\"\")\n",
    "    \n",
    "    # Load models\n",
    "    try:\n",
    "        feature_extractor, classifier, scaler, class_mapping, device = load_models()\n",
    "        st.success(\"Models loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading models: {e}\")\n",
    "        st.stop()\n",
    "    \n",
    "    # File upload\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"Upload Cell Image\")\n",
    "        uploaded_file = st.file_uploader(\n",
    "            \"Choose an image...\",\n",
    "            type=[\\'bmp\\', \\'png\\', \\'jpg\\', \\'jpeg\\']\n",
    "        )\n",
    "        \n",
    "        if uploaded_file is not None:\n",
    "            image = Image.open(uploaded_file).convert(\\'RGB\\')\n",
    "            st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"Classification Results\")\n",
    "        \n",
    "        if uploaded_file is not None:\n",
    "            if st.button(\"Classify\", type=\"primary\"):\n",
    "                with st.spinner(\"Analyzing...\"):\n",
    "                    pred_class, probs, idx_to_class = predict(\n",
    "                        image, feature_extractor, classifier, scaler, class_mapping, device\n",
    "                    )\n",
    "                \n",
    "                # Display prediction\n",
    "                st.success(f\"**Predicted Class:** {pred_class.replace(\\'_\\', \\' \\').title()}\")\n",
    "                \n",
    "                # Probability chart\n",
    "                st.subheader(\"Class Probabilities\")\n",
    "                prob_data = {\n",
    "                    idx_to_class[i].replace(\\'_\\', \\' \\').title(): probs[i]\n",
    "                    for i in range(len(probs))\n",
    "                }\n",
    "                st.bar_chart(prob_data)\n",
    "                \n",
    "                # Risk assessment\n",
    "                abnormal_classes = [\\'carcinoma_in_situ\\', \\'light_dysplastic\\', \n",
    "                                   \\'moderate_dysplastic\\', \\'severe_dysplastic\\']\n",
    "                \n",
    "                if pred_class in abnormal_classes:\n",
    "                    st.warning(\"‚ö†Ô∏è **Abnormal cell detected.** Please consult a medical professional.\")\n",
    "                else:\n",
    "                    st.info(\"‚úÖ Cell appears normal.\")\n",
    "        else:\n",
    "            st.info(\"Please upload an image to classify.\")\n",
    "    \n",
    "    # Footer\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\n",
    "        \"**Disclaimer:** This tool is for research purposes only. \"\n",
    "        \"Always consult qualified medical professionals for diagnosis.\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save streamlit app\n",
    "with open(\"streamlit_app.py\", 'w') as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print(\"Streamlit dashboard saved as 'streamlit_app.py'\")\n",
    "print(\"\\nTo run the dashboard:\")\n",
    "print(\"  1. Install streamlit: pip install streamlit\")\n",
    "print(\"  2. Run: streamlit run streamlit_app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Data**: Successfully loaded and preprocessed the Herlev dataset with 7 cell classes\n",
    "\n",
    "2. **Baseline Models**: Evaluated VGG16, DenseNet121, and ResNet50 as standalone classifiers\n",
    "\n",
    "3. **Hybrid Model**: Implemented ResNet50 (frozen) + Logistic Regression which minimizes overfitting risk\n",
    "\n",
    "4. **Explainability**: Added SHAP analysis to understand feature importance\n",
    "\n",
    "5. **Dashboard**: Created Streamlit app for easy deployment and use\n",
    "\n",
    "### Files Generated:\n",
    "- `saved_models/resnet50_feature_extractor.pth` - Feature extractor weights\n",
    "- `saved_models/logistic_classifier.pkl` - Trained classifier\n",
    "- `saved_models/scaler.pkl` - Feature scaler\n",
    "- `saved_models/class_mapping.pkl` - Class name mapping\n",
    "- `saved_models/model_comparison_results.csv` - Performance comparison\n",
    "- `streamlit_app.py` - Interactive dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAll steps successfully implemented:\")\n",
    "print(\"  ‚úì Data loading and preparation\")\n",
    "print(\"  ‚úì Baseline model evaluation (VGG16, DenseNet121, ResNet50)\")\n",
    "print(\"  ‚úì Hybrid model implementation (ResNet50 + Logistic Regression)\")\n",
    "print(\"  ‚úì Final evaluation and comparison\")\n",
    "print(\"  ‚úì SHAP explainability\")\n",
    "print(\"  ‚úì Streamlit dashboard\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
